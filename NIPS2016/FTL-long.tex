\documentclass[english]{article}
% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2016}
%\usepackage{fullpage}

\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage[backgroundcolor = White,textwidth=2cm]{todonotes}
\usepackage[disable,backgroundcolor = White,textwidth=\marginparwidth]{todonotes}
\newcommand{\todoc}[2][]{\todo[color=Apricot!20,size=\tiny,#1]{C: #2}}
\newcommand{\todot}[2][]{\todo[color=Cerulean!20,size=\tiny,#1]{T: #2}}
\newcommand{\todoa}[2][]{\todo[color=Purple!20,size=\tiny,#1]{A: #2}}
\newcommand{\todor}[2][]{\todo[color=Blue!10,size=\tiny,#1]{R: #2}}

\usepackage{comment}
\usepackage{babel}
%\usepackage[round]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}

%\usepackage{hyperref}       % hyperlinks
\usepackage[pdftex,letterpaper=true,pagebackref=false]{hyperref} % with basic options
		% N.B. pagebackref=true provides links back from the References to the body text. This can cause trouble for printing.
\hypersetup{
    plainpages=false,       % needed if Roman numbers in frontpages
    pdfpagelabels=true,     % adds page number as label in Acrobat's page count
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
%    pdftitle={(Bandit) Convex Optimization with Biased Noisy Gradient Oracles},    % set
    pdfauthor={},    % set
%     pdfauthor={D\'avid Szepesv\'ari},    % set
%    pdfsubject={Subject},  % subject: CHANGE THIS TEXT! and uncomment this line
%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords, and uncomment this line if desired
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=blue,         % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{epstopdf}
\usepackage{esvect}
\usepackage{graphicx,wrapfig,lipsum,framed}

\usepackage{ntheorem}
\usepackage{thm-restate}
\usepackage[capitalize]{cleveref}
\usepackage{jmlr2e}


\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\tcF}{\widetilde{\cF}}
\newcommand{\hP}{\hat{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Exp}[1]{\mathbb{E}\left[ #1 \right]} 
\newcommand{\Expc}[2]{\mathbb{E}\left[ \left. #1 \right| #2 \right]} 
\newcommand{\ind}{\mathbb{I}}
\newcommand{\one}[1]{\ind\left(#1\right)}
\newcommand{\seto}[1]{\left\{#1\right\}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bS}{\mathbb{S}}
\newcommand{\inpro}[2]{\langle #1, #2\rangle}
\newcommand{\inprol}[2]{\left\langle #1, #2\right\rangle}
\newcommand{\ip}[1]{\langle#1\rangle}
\newcommand{\inner}[1]{\left\langle#1\right\rangle}
\newcommand{\KL}{\operatorname{KL}}
\newcommand{\set}[2]{\left\{#1 \,\vert\, #2 \right\}}
\newcommand{\lt}{\ell_t}
\newcommand{\ttheta}{\tilde{\theta}}
\newcommand{\wtheta}{\widehat{\theta}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\what}[1]{\widehat{#1}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\bd}{\mathrm{bd}}
\newcommand{\inangle}[2]{(#1,#2)}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\uD}{\overline{D}}
\newcommand{\lD}{\underline{D}}
\newcommand{\ra}{\rightarrow}
\newcommand{\eg}{\text{e.g. }}
\newcommand{\Prob}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\Probc}[2]{\mathbb{P}\left[\left. #1 \, \right| #2\right]}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\diag}{diag}

\newenvironment{proofof}[1]{\par\noindent{\bf Proof of #1\ }}{\hfill\BlackBox\\[2mm]}
%\newtheorem{theorem}[theorem]{Theorem}%[section]
%\newtheorem{lemma}[thm]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{corollary}[thm]{Corollary}
%%\newtheorem{comm}[thm]{Comment}
%\newtheorem{example}[thm]{Example}
%\newtheorem{remark}[thm]{Remark}



\title{
Following the Leader and
Fast Rates in Linear Prediction:
Curved Constraint Sets and Other Regularities\thanks{An earlier version of this paper was published in NIPS 2016 \citep{HuLaGySz16}.}%\thanks{R. Huang and Cs. Szepesv\'ari are with the Department of Computing Science,
% University of Alberta, AB, Canada, email: \texttt{ruitong@ualberta.ca},  \texttt{szepesva@ualberta.ca}. T. Lattimore was with the 
%School of Informatics and Computing, Indiana University, IN, USA, email: \texttt{tor.lattimore@gmail.com}. A. Gy\"orgy is
%with the Department of Electrical and Electronic Engineering, Imperial College London, UK, email: \texttt{a.gyorgy@imperial.ac.uk}.
%}
}

\ShortHeadings{Following the Leader and Fast Rates in Linear Prediction}{Huang, Lattimore, Gy\"orgy, and Szepesv\'ari}



 \editor{}

\author{\name Ruitong Huang \email ruitong@ualberta.ca \\
  \addr Department of Computing Science\\
  University of Alberta\\
  Edmonton T6G 2E8, Canada 
 \AND
 \name Tor Lattimore \email tor.lattimore@gmail.com \\
 \addr Indpendent Researcher \\
 Canberra, Australia
  \AND
  \name Andr\'as Gy\"orgy \email  a.gyorgy@imperial.ac.uk\\
  \addr Department of Electrical and Electronic Engineering\\
  Imperial College London\\
  South Kensington Campus, London SW7 2BT, UK 
  \AND
  \name Csaba Szepesv\'ari \email szepesva@cs.ualberta.ca \\
  \addr Department of Computing Science\\
  University of Alberta\\
  Edmonton T6G 2E8, Canada
}

%\author{Ruitong Huang \and Tor Lattimore \and Andr\'as Gy\"orgy \and Csaba Szepesv\'ari}

\begin{document}

\maketitle

\begin{abstract}
The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms,
is known to perform well when the loss functions it is used on are convex and positively curved.
In this paper we ask whether there are other ``lucky'' settings when FTL achieves sublinear, ``small'' regret.
In particular, we study the fundamental problem of linear prediction over a  convex, compact domain with non-empty interior.
Amongst other results, we prove that the curvature of  the boundary of the domain can act as if the losses
were curved: In this case, which precisely happens if and only if the domain is strongly convex, we prove that as long as 
the mean of the loss vectors have positive lengths bounded away from zero, 
FTL enjoys a logarithmic growth rate of regret, while, for example, for polytope domains and stochastic data it enjoys
finite expected regret.
The former result is also extended to strongly convex domains by establishing an equivalence between the strong convexity of sets and the minimum curvature of their boundary,
which could be of independent interest.
Building on a previously known meta-algorithm, we also get
 an algorithm that simultaneously enjoys the worst-case guarantees and the smaller regret of FTL when the data is ``easy.''
 We also show that such guarantees are achievable directly (e.g., by the follow the regularized leader algorithm or by a shrinkage-based variant of FTL)
 when the constraint set is the unit ball.
%while it approaches the 
\end{abstract}

\begin{keywords}
online linear optimization, follow the leader, logarithmic regret, strongly convex decision set, curvature
\end{keywords} 

\section{Introduction}
Learning theory traditionally has been studied in a statistical framework, discussed at length, for example, by \citet{SSS14:book}.
The issue with this approach is that the analysis of the performance of learning methods seems to critically depend
on whether the data generating mechanism satisfies some probabilistic assumptions. 
Realizing that these assumptions are not necessarily critical, much work has been devoted recently to 
studying learning algorithms in the so-called online learning framework \citep{CBLu06:book}. %SSS14:book}.
The online learning framework makes minimal assumptions about the data generating mechanism,
while allowing one to replicate results of the statistical framework through online-to-batch conversions
\citep{CBCoG04:OnlineToBatch}.
By following a minimax approach, however, results proven in the online learning setting, at least initially, led to rather
conservative results and algorithm designs, failing to capture how more regular, ``easier'' data, may give rise to
faster learning speed. This is problematic as it may suggest overly conservative learning strategies, missing
opportunities to extract more information when the data is nicer. Also, it is hard to argue that data resulting from
passive data collection, such as weather data, would ever be adversarially generated (though it is equally hard
to defend that such data satisfies precise stochastic assumptions).
Realizing this issue, during recent years much work has been devoted to understanding what regularities and how can lead to 
faster learning speed.
For example, much work has been devoted to showing that faster learning speed (smaller ``regret'') can be achieved
in the online convex optimization setting when the loss functions are ``curved'', such 
as when the loss functions are strongly convex or exp-concave, %and on how to adaptively exploit this property,
or when the losses show small variations, or the best prediction in hindsight has a small total loss, and that these properties can be exploited in an adaptive manner  (e.g.,
\citealt{MF92}, \citealt{FrSc97},
\citealt{gaivoronski2000stochastic},
\citealt{CBLu06:book},
\citealt{hazan2007logarithmic},
\citealt{bartlett2007adaptive},
\citealt{kakade2009mind},
\citealt{orabona2012beyond},
\citealt{RakhlinS13},
\citealt{vanerven2015fast},
\citealt{foster2015adaptive}).
%Huge literature on adaptive and fast rates, impossible to summarize here (and out of scope), but, e.g., 
%\citet{orabona2012beyond,vanerven2015fast,foster2015adaptive} review and unify many previous approaches.

In this paper we contribute to this growing literature by studying online linear prediction and the follow the leader (FTL) algorithm.
Online linear prediction is arguably the simplest yet fundamental of all the learning settings, and lies at the heart of online
convex optimization, while it also serves as an abstraction of core learning problems such as prediction with expert advice.
FTL, the online analogue of empirical risk minimization of statistical learning, is the simplest learning strategy, one can think of.
Although the linear setting of course removes the possibility of exploiting the curvature of losses, as we will see, there are
multiple ways online learning problems can present data that allows for small regret, even for FTL.
As is it well known, in the worst case,
FTL suffers a linear regret (e.g., Example 2.2 of \citealt{SS12:Book}). 
However, for ``curved'' losses (e.g., exp-concave losses), FTL was shown to achieve small (logarithmic) regret
(see, e.g., \citealt{MF92,CBLu06:book,gaivoronski2000stochastic,hazan2007logarithmic}).

In this paper we take a thorough look at FTL in the case when the losses are linear, 
but the problem perhaps exhibits other regularities.
The motivation comes from the simple observation that, for prediction over the simplex, when
the loss vectors are selected independently of each other from a distribution with a bounded support with a
nonzero mean, FTL quickly locks onto selecting the loss-minimizing vertex of the simplex, achieving finite expected regret.
In this case, FTL is arguably an excellent algorithm.
In fact, FTL is shown to be the minimax optimizer for the binary losses in the  stochastic expert setting in the paper of \citet{kotlowskiminimax}.
Thus, we ask the question of whether there are other regularities that allow FTL 
to achieve nontrivial performance guarantees.
Our main result shows that when the decision set (or constraint set) has a sufficiently ``curved'' boundary (or equivalently, if it is strongly convex) and the linear loss is bounded away from $0$, FTL 
is able to achieve logarithmic regret even in the adversarial setting, thus opening up a new
way to prove fast rates not based on the curvature of losses, but on that of the boundary of the constraint set and non-singularity of the linear loss.
In a matching lower bound we show that this regret bound is essentially unimprovable.
%As we will show in an essentially matching lower bound, this regret bound must increase with the inverse
%of the norm of the mean of losses (i.e., small means makes the regret bound explode).
We also show an alternate bound for polytope constraint sets, which allows us to prove that 
(under certain technical conditions) for stochastic problems the expected regret of FTL will be finite.
To finish, we use $(\cA,\cB)$-prod of \citet{sani2014exploiting} to design an algorithm 
that adaptively interpolates between the worst case $O(\sqrt{n\log n})$ regret and the smaller regret bounds,
which we prove here for ``easy data.'' We also show that if the constraint set is the unit ball, both the follow the regularized leader (FTRL) algorithm and a combination of FTL and shrinkage, which we call follow the shrunken leader (FTSL), achieve logarithmic regret for easy data. Simulation results on artificial data complement the theoretical findings. 
%to illustrate the theory

While we believe that we are the first to point out that the curvature of the constraint set $\cW$ can help in speeding up learning,
this effect is known in convex optimization since at least the work of  \citet{LePo66},
who showed that exponential rates are attainable for strongly convex constraint sets if the norm of the gradients of the objective function admit a uniform lower bound. \todoc{Longer version: This is their Theorem 6.1, part (5).}
More recently, \citet{garber2014faster} %removes a restricting technical condition assumed by \citet{LePo66} and
proved an $O(1/n^2)$ optimization error bound (with problem-dependent constants) for the Frank-Wolfe algorithm for strongly convex and smooth objectives and strongly convex constraint sets.
The effect of the shape of the constraint set was also discussed by \citet{abbasi2010forced} who demonstrated $O(\sqrt{n})$ regret in the linear bandit setting.
While these results at a high level are similar to ours, our proof technique is rather different than that used there.
% that the curvature of the constraint set leads to faster convergence
%in the Frank-Wolfe algorithm
%for smooth, strongly convex functions, using Frank-Wolfe, $O(1/t^2)$ optimization error is attainable after $t$ iterations.
  
\todoc[inline]{
Interpolating between stochastic and adversarial settings: \citet{bubeck2012best}.
I think Rakhlin and Karthrik also write about this. What did they write? Cite them.}
\todor[inline]{\citep{abernethy2008optimal} section 4.2 talks about lower bound for linear game with constraint sets being balls. \citep{abernethy2009stochastic} relates the regret to the flatness of $\Phi$ and the Bregman divergence. \citep{abernethy2014online} Bregman divergence again.}
  
\todoc[inline]{
\citet{MF92}  considers the following assumption (dropping measurability and other technical requirements):
Let $\ell: \cF \times \cW \to [0,\infty)$ be a fixed loss function.
For a probability distribution $P$ over $\cF$, let $w^*(P) = \argmin_{w\in \cW} \int \ell(f,w) P(df)$.
Further, for $\alpha\in [0,1]$, $f\in \cF$, let $P_{\alpha,x} = P+ \alpha( \delta_f - P)$, where $\delta_f$ is the Dirac measure that puts all the weight to $f$. (Note that $P_{\alpha,x}-P = \alpha (\delta_f-P)$.)
Then, the assumption is that for some $L>0$ and for all $f\in \cF$,
 $|\ell(f, b^*(P) ) - \ell( f, b^*(P_{\alpha,f}))| \le \alpha L$ (a form of a Lipschitz condition).
Under this assumption they show that FTL achieves logarithmic regret.
How does this assumption relate to our smoothness assumption?
}
\todor[inline]{More about stability. \citep{saha2012interplay}. Such stability is usually achieved by the strongly convexity of the loss function.}  
 

%\vspace{-0.05cm}
\section{Preliminaries, online learning and the follow the leader algorithm}
\label{sec:notation}
% \begin{wrapfigure}{r}{6cm}
%	\vspace{-.5cm}
%	\centering
%	\begin{algorithmic}[1]
%		\FOR{$t = 1 \text{ to } n$}
%		\STATE Learner predicts $w_t\in \cW$;
%		\STATE Environment picks $\ell_t\in \cL$;
%		\STATE Learner suffers $\ell_t(w_t)$ and learns $\ell_t$.
%		\ENDFOR
%	\end{algorithmic}
%	\caption{Online Learning} 
%  	\label{fig:onlinelearning}
%	\vspace{-.5cm}
%\end{wrapfigure} 
We consider the standard framework of online convex optimization, where a learner and an environment interact in a sequential manner in $n$ rounds: In  every round $t=1,\ldots,n$, first the learner predicts $w_t\in \cW$. Then the environment picks a loss function $\ell_t\in \cL$, and the learner suffers loss $\ell_t(w_t)$ and observes $\ell_t$. 
%as shown in \cref{fig:onlinelearning}.
%as follows \citep{Zin03}:
Here, $\cW$ is a  compact convex subset of $\real^d$ with non-empty interior and
 $\cL$ is a set of convex functions, mapping $\cW$ to the reals.
 The elements of $\cL$ are called loss functions.
The performance of the learner is measured in terms of its regret,
\[
R_n = \sum_{t=1}^n \lt(w_t) - \min_{w\in \cW}\sum_{t=1}^n \lt(w)\,.
\]
 
The simplest possible case, which will be the focus of this paper,
is when the losses are linear, that is, when $\lt(w) = \ip{f_t,w}$ for some $f_t\in \cF\subset \real^d$.
\newcommand{\tlt}{\tilde{\ell}_t}
In fact, the linear case is not only simple, but is also fundamental since the case of nonlinear loss functions can be reduced to it: Indeed, even if the losses are nonlinear, 
defining $f_t \in \partial \lt(w_t)$ to be a subgradient%
\footnote{
We let $\partial g(x)$ denote the subdifferential of a convex function $g:\dom(g) \to \R$ at $x$,
that is, $\partial g(x) = \set{\theta\in \R^d}{g(x') \ge g(x) + \ip{\theta, x'-x} \,\, \forall x'\in \dom(g) }$,
where $\dom(g)\subset \R^d$ is the domain of $g$.
} 
of $\lt$ at $w_t$ and  letting $\tlt(u) = \ip{f_t,u}$, by the definition of subgradients,
$\lt(w_t)-\lt(u) \le \lt(w_t)-(\lt(w_t)+\ip{f_t,u-w_t}) = \tlt(w_t)-\tlt(u)$, hence for any $u\in \cW$,
\[
\sum_t \lt(w_t) - \sum_t \lt(u) \le \sum_t \tilde{\lt}(w_t) - \sum_t \tilde{\lt}(u)\,.
\]
In particular, if an algorithm keeps the regret small no matter how the linear losses are selected
(even when allowing the environment to pick losses based on the choices of the learner), 
the algorithm can also be used to keep the regret small in the nonlinear case. 

Hence, in what follows we will study the linear case $\ell_t(w)=\ip{f_t,w}$ and, in particular, we will study the regret
of the so-called ``Follow The Leader'' (FTL) learner, which, in round $t\ge 2$ 
picks
% $w_t$ such that $w_t$ minimizes the 
%total loss $\sum_{i=1}^{t-1} \ell_i(w)$ accumulated so far, cf. \cref{fig:ftl}
\begin{align*}
w_t = \argmin_{w\in \cW} \sum_{i=1}^{t-1} \ell_i(w)\,.
\end{align*}
For the first round, $w_1\in \cW$ is picked in an arbitrary manner.
When $\cW$ is compact, the optimal $w$ of $\min_{w\in\cW} \sum_{i=1}^{t-1}\inpro{w}{f_t}$ is attainable,
which we will assume henceforth.
If multiple minimizers exist, we simply fix one of them as $w_t$.
We will also assume that $\cF$ is non-empty, compact and convex. \todoc{Why compact? Convex? How is this used?}\todoa{It is used with $\Phi$ and its Bregman divergence. Could be relaxed but this is the standard way.}

One problem of the linearization technique is that if some algorithm's performance depends on some additional properties of the linear loss function, linearization may not preserve these and could lead to suboptimal performance. For example, if the loss functions are strongly convex and the optimum in hindsight (in fact, $w_{n+1}$) is an inner point of $\cW$, FTL has no chance to do well, since it will always predict points on the boundary.  Thus, while our results extend from linear losses to arbitrary convex functions, some of the conditions of our regret bounds may be violated or the constants in the bounds might blow up, possibly leading to trivial or weak regret bounds. Thus, in practice, one should always check if the linearization step makes sense. On the positive side, no problem occurs if the optimum is outside of $\cW$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\if0
 \begin{wrapfigure}{R}{5cm}
	\vspace{-.5cm}
	\centering
	\begin{algorithmic}
		\STATE In round $t\ge 2$, predict:
		\begin{align*}
		w_t = \argmin_{w\in \cW} \sum_{i=1}^{t-1} \ell_i(w)\,.
		\end{align*}
		while in round one predict arbitrarily.
	\end{algorithmic}
	\caption{Follow the Leader (FTL)} 
  	\label{fig:ftl}
	\vspace{-.5cm}
\end{wrapfigure} 
\fi

\subsection{Support functions}
Let $\Theta_t = -\frac1t \sum_{i=1}^t f_i$ be the negative average of the first $t$ vectors
in $(f_t)_{t=1}^n$, $f_t\in \cF$.
For convenience, we define $\Theta_0 := 0$.
Thus, for $t\ge 2$,
\begin{align*}
w_t =  \argmin_{w\in\cW} \sum_{i=1}^{t-1} \ip{ w, f_i } = \argmin_{w\in\cW} \ip{ w, -\Theta_{t-1} }
= \argmax_{w\in \cW} \ip{w,\Theta_{t-1}}\,.
\end{align*}
Denote by $\Phi(\Theta) = \max_{w\in\cW} \langle w, \Theta\rangle$ the so-called \emph{support function} of $\cW$. 
The support function, being the maximum of linear and hence convex functions, is itself convex.
Further $\Phi$ is positive homogenous: for $a\ge 0$ and $\theta\in \R^d$, $\Phi(a \theta) = a\Phi(\theta)$.
It follows then that the epigraph $\epi(\Phi) = \set{ (\theta,z)}{ z\ge \Phi(\theta), z\in \R, \theta\in \R^d }$ of $\Phi$ is a cone,
since for any $(\theta,z)\in \epi(\Phi)$ and $a\ge 0$, 
$az \ge a \Phi(\theta) = \Phi(a\theta)$, $(a\theta,az)\in \epi(\Phi)$ also holds.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The differentiability of the support function is closely tied to whether in the FTL algorithm the choice of $w_t$ is
uniquely determined:
\begin{proposition} 
\label{prop:derivativePhi}
Let $\cW\ne \emptyset$ be convex and closed.
Fix $\Theta$ and let $\cZ:= \set{w\in \cW}{\inpro{w}{\Theta} =  \Phi(\Theta) }$.
Then, $\partial \Phi(\Theta) = \cZ$ and, in particular,
$\Phi(\Theta)$ is differentiable at $\Theta$ if and only if 
$\max_{w\in\cW} \inpro{w}{\Theta}$ has a unique optimizer.
In this case, $\nabla \Phi(\Theta) = \argmax_{w\in \cW} \ip{w,\Theta}$.
\end{proposition}
The proposition follows from Danskin's theorem when $\cW$ is compact
(e.g., Proposition B.25 of \citealt{bertsekas99nonlinear}), 
but a simple
direct argument, presented in \cref{sec:pr-prop:derivativePhi} for completeness, can also be used to show that it also remains true even when $\cW$ is unbounded.
By \cref{prop:derivativePhi},
when $\Phi$ is differentiable at $\Theta_{t-1}$,
$w_t = \nabla \Phi(\Theta_{t-1})$.
%\todoa{This part is somewhat problematic, as $\phi(\Theta)$ is not defined for some $Theta$ if 

\subsection{A motivating example}
\label{sec:FTLstoch}

\begin{wrapfigure}{r}{0.4\textwidth}
	\vspace{-.7cm}
\begin{framed}
	\centering
	\includegraphics[width = \textwidth,
	trim={6.2cm 3.5cm 1.5cm 0},clip]
	{figures/ExcessError}
	\vspace{-0.4cm}
	\caption{Illustration of how fast rates can be achieved by FTL.
	}
	\label{fig:excesserror}
	\vspace{-0.1cm}
\end{framed}
	\vspace{-1cm}
\end{wrapfigure} 
%
We close this section by an example demonstrating how fast rates can be achievable by the FTL algorithm. Consider the case when the losses are independent and identically distributed (i.i.d.),
that is, $(f_t)$ is an i.i.d.\ sequence with expectation $\mu$. Then  $\Exp{\Theta_t} = -\mu$, and we have $\norm{\Theta_t +\mu}_2 = O(1/\sqrt{t})$ with high probability. 
Thus, say, for $\cW$ being the unit ball of $\R^d$, one has $w_t = \Theta_t/\norm{\Theta_t}_2$, and, therefore,
a crude bound suggests that $\norm{w_t-  w^* }_2 = O(1/\sqrt{t})$ where $w^*$ is the optimal decision in hindsight, overall predicting that $\Exp{R_n} = O(\sqrt{n})$.
On the other hand, in the rest of the paper we provide conditions when the expected regret can be much smaller than this. Below we give a simple geometric explanation how it can happen.

Let $\cW = \set{w}{\|w\|_2\le 1}$ and consider a stochastic setting where the $f_i$ are i.i.d. samples %from some underlying distribution
with expectation $\Exp{f_i} = \mu = (-1,0,\ldots,0)$ and $\|f_i\|_\infty\le M$.
It is straightforward to see that $w^* = (1,0,\ldots,0)$, and thus $\inpro{w^*}{\mu} = -1$. Let $\mu_t=-\Theta_t$ denote our estimate of $\mu$ after $t$ time steps; then $\|\mu_t-\mu\| = O(1/\sqrt{t})$
with high probability.
%Let $E = \set{-\theta}{\|\theta - \mu\|_2 \le \epsilon}$. As suggested beforehand, we expect $-\mu_t\in E$ with high probability fro $t$ large enough.
Now consider \cref{fig:excesserror}: The origin is denoted by $O$, the optimal prediction $w^*=-\mu$ by $D$, and $-\mu_t$ by $\hat{A}$. Then the prediction of 
FTL at time $t$ is $\tilde{A}$, the intersection of the line connecting $O$ and $\hat{A}$ with the unit sphere, and its instantaneous excess loss is
$\inpro{\vv{O\tilde{A}}}{\vv{OD}} - 1 = |\overline{\tilde{B}D}|$ where $\tilde{B}$ is the orthogonal projection of $\tilde{A}$ to $\overline{OD}$. 
Next we give a simple geometric argument showing that if $|\overline{\hat{A}D}| \le \epsilon$ then $|\overline{\tilde{B}D}| \le \epsilon^2$. Since 
$|\overline{\hat{A}D}|=\|\mu_t - \mu\| = O(1/\sqrt{t})$ with high probability, this means that the excess error at time $t$ is $O(1/t)$, making the regret $O(\log n)$ %(with high probability) 
in $n$ time steps, much smaller than the 
previously anticipated $O(\sqrt{n})$ regret. 
To finish, let  $A$ denote the orthogonal projection of $D$ to the line connecting $O$ and $\hat{A}$; then the Pythagorean theorem implies that $|\overline{OA}| \le |\overline{OD}|=|\overline{O\tilde{A}}|$, and so
$A \in \overline{O\tilde{A}}$. Therefore, the orthogonal projection of $A$ to $\overline{OD}$, denoted by $B$, belongs to the segment $\overline{O\tilde{B}}$,
and so $|\overline{BD}| \ge |\overline{\tilde{B}D}|$. Since the triangles $OAD$ and $ABD$ are similar, we have $\frac{|\overline{BD}|}{|\overline{AD}|} = \frac{|\overline{AD}|}{|\overline{OD}|}$. Therefore, 
$|\overline{BD}| \le |\overline{AD}|^2 \le |\overline{\hat{A}D}|^2$ (by the definition of $A$), implying $|\overline{\tilde{B}D}| \le |\overline{\hat{A}D}|^2$, which we wanted to prove.

\if0
On the other hand, the prediction of ERM $\what{w_n} = \frac{-\mu_n}{\|\mu_n\|_2}$, where $\mu_n = \frac{1}{n}\sum_{i=1}^n f_i$.
By Hoeffding's inequality, 
\[
\Prob{ \|\mu_n - \mu\|_2 \ge \epsilon } \le 2d\exp\left(-\frac{n\epsilon^2}{2dM^2}\right).
\]

Moreover, we further show in the appendix that for $\what{w} = \frac{-\mu_n}{\|\mu_n\|_2}$ and $\|\mu_n - \mu\|_2 \le \epsilon$, 
\[
\inpro{\what{w}}{\mu} - \inpro{w^*}{\mu} \le \epsilon^2. \numberthis\label{eq:exampleloss}
\]
Therefore,
\[
\xi = \Exp{\inpro{\hat{w}_n}{\mu} - \inpro{w^*}{\mu}} \le \int_{0}^{\infty} \epsilon^2 c_1\exp\left(-\frac{n\epsilon^2}{c_2}\right) \text{d}\epsilon = \frac{c_1}{n}\int_{0}^{\infty} \epsilon^2\exp\left(-\frac{\epsilon^2}{c_2}\right) \text{d}\epsilon = O(\frac{1}{n}).
\]
\fi

\section{Non-stochastic analysis of FTL}
\label{sec:FTL}
We start by rewriting the regret of FTL in an equivalent form, which shows that we can expect FTL to enjoy a small
regret when successive weight vectors move little. 
A noteworthy feature of the next proposition is that rather than bounding the regret from above, it gives an equivalent
expression for it. 
\begin{proposition}
\label{prop:regretabel}
The regret $R_n$ of FTL satisfies
\begin{align*}
R_n & =  \sum_{t=1}^n t\,\ip{ w_{t+1}-w_t,\Theta_t}  \,.
\end{align*}
\end{proposition}
The result is a direct corollary of Lemma 9 of \citet{McMahan10:Equiv}, which holds 
for any sequence of losses, even in the lack of convexity.
It is also a tightening of the well-known inequality $R_n \le \sum_{t=1}^n \ell_t(w_t)-\ell_t(w_{t+1})$,
which again holds for arbitrary loss sequences (e.g., Lemma 2.1 of \citet{SS12:Book}).
To keep the paper self-contained, we give an elegant, short direct proof, based on the summation by parts formula:
\begin{proof}
The summation by parts formula states that for any $u_1,v_1,\dots,u_{n+1},v_{n+1}$ reals,
$
\sum_{t=1}^n u_t\,(v_{t+1}-v_t) = (u_{t+1}v_{t+1}-u_1 v_1) - \sum_{t=1}^n (u_{t+1}-u_t)\,v_{t+1} 
$.
Applying this to the definition of regret
with $u_t:=w_{t,\cdot}$ and $v_{t+1} := t\Theta_{t}$, we get
\begin{align*}
R_n 
& = -\sum_{t=1}^n \ip{w_t,t\Theta_t - (t-1)\Theta_{t-1}} + \ip{w_{n+1},n\Theta_n}   \\
& = - \left\{ 
		\bcancel{\ip{w_{n+1},n\Theta_n}} - 0 - \sum_{t=1}^n \ip{w_{t+1}-w_t,t\Theta_t}  \right\} +
		\bcancel{\ip{w_{n+1},n\Theta_n}}.
\end{align*}
\end{proof}

Our next proposition gives another formula that is equal to the regret.
Although this formula is not directly needed for the rest of the paper, it provides interesting insights:
as opposed to the previous result, it is independent of $w_t$, and 
directly connects the sequence $(\Theta_t)_t$ to the 
geometric properties of $\cW$ through the support function $\Phi$.
A similar expression for a general ``Follow the Regularized Leader'' algorithm was also derived by \citet{abernethy2014online}.
For this proposition, we will momentarily assume that $\Phi$ is differentiable at $(\Theta_t)_{t\ge 1}$.
%;a more general statement will follow later.
%To state the proposition, recall that the Bregman divergence from $u$ to $v$ induced by a convex function $g$ 
%which is differentiable at $u$ is $D_g(v,u) = g(v) - g(u) - \ip{\nabla g(u), v-u}$.
%When $\Phi$ is differentiable at $\Theta_{t-1} (\neq0)$, $w_t = \nabla \Phi(\Theta_{t-1})$.


%The next proposition shows that the regret of FTL is in fact tied to the smoothness of the function $\Phi$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition} 
\label{prop:R_nBregmanDivergence}
Assume $\Phi$ is differentiable at $\Theta_1, \ldots, \Theta_n$. Then %\todoc{There is an issue that $\Phi$ is not differentiable at $\Theta_0 = 0$.}
\begin{align}
\label{eq:regreteq}
R_n = \sum_{t=1}^{n} t\,D_{\Phi}(\Theta_t,\Theta_{t-1})\,,
\end{align}
where $D_{\Phi}(\theta', \theta) = \Phi(\theta') - \Phi(\theta) - \ip{ \nabla\Phi(\theta), \theta' - \theta}$ is the Bregman divergence of $\Phi$
and
we use the convention that $\nabla\Phi(0) = w_1$.
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
Let $v = \argmax_{w\in\cW}\inpro{w}{\theta}$, 
$v' = \argmax_{w\in \cW}\ip{w,\theta'}$.
When $\Phi$ is differentiable at $\theta$,
\begin{align}
D_{\Phi}(\theta', \theta) & = \Phi(\theta') - \Phi(\theta) - \inpro{\nabla\Phi(\theta)}{\theta' \!- \theta} 
  =  \inpro{v'}{\theta'} \!- \inpro{v}{\theta} -\inpro{v}{\theta' \!- \theta} = \inpro{v'\!-v}{\theta'}\,. 
\label{eq:bregman}
\end{align}
Therefore, by \cref{prop:regretabel}, $R_n = \sum_{t=1}^{n} t\ip{ w_{t+1}-w_t,\Theta_t} = \sum_{t=1}^{n} t\,D_{\Phi}(\Theta_t,\Theta_{t-1})$.
\end{proof}

When $\Phi$ is non-differentiable at some of the points $\Theta_1,\dots,\Theta_n$, the equality in the above proposition can be replaced with inequalities.
Defining the upper Bregman divergence 
$\uD_{\Phi}(\theta', \theta) 
= \sup_{w\in \partial \Phi(\theta)} \Phi(\theta') - \Phi(\theta) - \ip{ w, \theta' - \theta}$ and the lower Bregman divergence $\lD_{\Phi}(\theta', \theta)$ similarly with $\inf$ instead of $\sup$, 
%
we can easily obtain an analogue of \Cref{prop:R_nBregmanDivergence}:
%These definitions give rise to
%a trivial extension of \eqref{eq:bregman}:
%$\lD_{\Phi}(\theta', \theta) \le \ip{v'-v,\theta'} \le \uD_{\Phi}(\theta', \theta)$. Combining with  \cref{prop:regretabel} as above, we obtain the following result:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{proposition} 
%\label{prop:R_nBregmanDivergence2}
%We have
\begin{align}
\label{eq:regreteq_alt}
\sum_{t=1}^{n} t\,\lD_{\Phi}(\Theta_t,\Theta_{t-1})
\le
R_n 
\le \sum_{t=1}^{n} t\,\uD_{\Phi}(\Theta_t,\Theta_{t-1})\,.
\end{align}
%\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The proof follows the same steps as the proof of the previous proposition, except that instead of \eqref{eq:bregman}
%we use
%when $w_t$ was replaced by $\nabla \Phi(\Theta_{t-1})$, now
%we just  keep $w_t$ up to the end, when we take the infimum (supremum)
%over all elements of $\partial \Phi(\Theta_{t-1})$ to get the lower (respectively, upper)
%bound, exploiting that $w_t \in \partial \Phi(\Theta_{t-1})$.

\subsection{Constraint sets with positive curvature}
\label{subsec:positiveCurvature}
The previous results show in an implicit fashion that the curvature of $\cW$ controls the regret. Before presenting our first main results, which make this connection explicit, we define some basic notions from differential geometry related to the curvature, while some extra details are presented in \cref{subsec:preliminariesDiffGeo} (all differential geometry concept and results that we need can be found in Section~2.5 of the book of \citealp{Sch14:ConvexBodies}).

\subsubsection{Curvature and strong convexity}
\label{sec:curvature_intro}
Given a twice continuously differentiable planar curve $\gamma$ in $\real^2$,\footnote{See \cref{sec:app_planar} for more details.} there exists a parametrization with respect to the curve length $s$, such that $\|\gamma'(s)\| = \|\left(x'(s), y'(s)\right)\| = \sqrt{x'(s)^2 + y'(s)^2}=1$. Under the curve length parametrization, the curvature of $\gamma$ at $\gamma(s)$ is $\|\gamma''(s)\|$.
Define the unit normal vector ${\bf n}(s)$ as the unit vector that is perpendicular to $\gamma'(s)$.\footnote{There exist two unit vectors that are perpendicular to $\gamma'(s)$ for each point on $\gamma$. Pick the ones that are consistently oriented.}
Note that ${\bf n}(s)\cdot \gamma'(s) = 0$. Thus $0=\left({\bf n}(s)\cdot \gamma'(s)\right)' = {\bf n}'(s)\cdot\gamma'(s) + {\bf n}(s)\cdot \gamma''(s)$, and $\|\gamma''(s)\| = \|{\bf n}(s)\cdot \gamma''(s)\| = \|{\bf n}'(s)\cdot\gamma'(s)\| = \|{\bf n}'(s)\|$. Therefore, the curvature of $\gamma$ at point $\gamma(s)$ is the length of the differential of its unit normal vector.
 
%{\bf Principal Curvature}\\
Denote the boundary of $\cW$ by $\bd(\cW)$ and a tangent plane of $\bd(\cW)$ at point $w$ by $T_w\cW$.
We shall assume that $\cW$ is twice continuously differentiable, that is, $\bd(\cW)$ is a twice continuously differentiable submanifold of $\R^d$. Then $T_w\cW$ is unique,
and there exists a unique unit vector at $w$ that is perpendicular to $T_w\cW$ and points outward of $\cW$.
In fact, one can define a continously differentiable normal unit vector field on $\bd(\cW)$, $u_{\cW}: \bd(\cW) \to \bS^{d-1}$, the so-called Gauss map, which maps a boundary point $w\in \bd(\cW)$ to the unique outer normal vector to $\cW$ at $w$, where
$\bS^{d-1}=\set{x\in\R^d}{\|x\|_2=1}$ denotes the unit sphere in $\R^d$.\footnote{See \cref{sec:app_manifold} for more details.}
Since $u_{\cW}(w)$ maps $\bd(\cW)$ to unit vectors, the differential of the Gauss map, $\nabla u_{\cW}(w)$, defines a linear endomorphism of $T_w\cW$. Moreover, $\nabla u_{\cW}(w)$ is a self-adjoint operator, with nonnegative eigenvalues.
The differential of the Gauss map, $\nabla u_{\cW}(w)$, describes the curvature of $\bd(\cW)$ via the second fundamental form. In particular, the \emph{principal curvatures} of $\bd(\cW)$ at $w\in\bd(\cW)$ are defined as the eigenvalues of $\nabla u_{\cW}(w)$.   
Perhaps a more intuitive, yet equivalent definition, is that the principal curvatures are the eigenvalues
of the Hessian of $f=f_w$ in the parameterization $t\mapsto w+t-f_w(t) u_{\cW}(w)$ of $\bd(\cW)$,
which is valid in a small open neighborhood of $w$, where $f_w: T_w \cW \to [0,\infty)$ is
a suitable convex, nonnegative valued function that also satisfies $f_w(0)= 0$ (see \cref{fig:diffgeo}\todoa{Move the two figures further}).
%%% and where $T_w \cW$, 
%%%a hyperplane of $\R^d$,
%%%denotes the tangent space of $\cW$ at $w$, 
%%%obtained by taking the support plane $H$ of $\cW$ at $w$ and shifting it by $-w$.
Thus, the principal curvatures at some point $w\in \bd(\cW)$ describe the local shape of $\bd(\cW)$ 
up to the second order. 
In this paper, we are interested in the minimum principal curvature at $w\in\bd(\cW)$, which can be interpreted as the minimum curvature at $w$ over all the planar curves $\gamma \in \bd(\cW)$ that go through $w$. 

\begin{figure}[th]
%\begin{wrapfigure}{R}{0.35\textwidth}
%	\vspace{-0.2cm}
%	\begin{framed}
	\centering
	\includegraphics[width=0.7\textwidth]{figures/DiffGeo}
	\caption{Some differential geometry notations. \label{fig:diffgeo}}
\end{figure}
%\end{framed}
%\vspace{-1cm}
%\end{wrapfigure}

%\subsubsection{Strongly convex sets and principal curvatures}
%\label{sec:sc}
%The results in the previous section depended on the minimum principal curvature of the constraint set $\cW$.
A related concept that has been used in convex optimization to show fast rates is that of a strongly convex constraint set \citep{LePo66,garber2014faster}:
$\cW$ is $\lambda$-strongly convex with respect to the norm $\norm{\cdot}$ if, for  any $x,y\in \cW$ and $\gamma\in [0,1]$, the $\norm{\cdot}$-ball with origin $\gamma x + (1-\gamma) y$ and radius $\gamma(1-\gamma) \lambda \norm{x-y}^2/2 $ is included in $ \cW$.
That is, for any $z\in \R^d$ with $\norm{z}=1$, $\gamma x + (1-\gamma) y + \gamma(1-\gamma) \frac{\lambda}{2} \norm{x-y}^2 z\in \cW$.
 %\footnote{$B(o,r)$ denotes the $\norm{\cdot}$-ball with origin $o$ and radius $r$.}
%We show in \cref{prop:strongconvex} in \cref{sec:sc} 
Next we show that a  convex body
$\cW$ with twice continuously differentiable boundary is $\lambda$-strongly convex with respect to $\norm{\cdot}_2$ if and only if the principal curvatures of the surface $\bd(\cW)$ are all at least $\lambda$.\footnote{Following \citet{Sch14:ConvexBodies}, a convex body in $\R^d$ is any compact, convex subset of $\R^d$ with non-empty interior.}
In the rest of the paper, $B_r(x)=\set{y \in \R^d}{ \|x-y\|_2 \le r}$ will denote the Euclidean ball of radius $r$ centered at $x$ (in case $x$ is the origin, it will often be omitted).

%\begin{restatable}{proposition}{FTLSCset}
\begin{proposition}
	\label{prop:strongconvex}
	Let $\cW \subset \R^d$ be a convex body with with twice continuously differentiable boundary and support function $\varphi$, %and assume that all principal curvatures of $\cW$ are non-zero 
	%\todoa{Can we rephrase this without the curvature? E.g., using $\varphi$?} \todor{Prove the equivalence by (i)->(ii)->(iii)->(i), then this condition can be removed.} 
	and let $\lambda$ be an arbitrary positive number.
	Then the following statements are equivalent:
	\begin{enumerate}[(i)]
		\item \label{sc:l1} The smallest principal curvature of $\cW$ is at least $\lambda$.
		\item \label{sc:l2} $\cW= {\large \cap}_{\theta \in \bS^{d-1} }{B_{1/\lambda} (w_\theta- \theta /\lambda)}$ where $w_\theta \in \partial \varphi(\theta) \subset \bd(\cW)$.
		\item \label{sc:l3} $\cW$ is $\lambda$-strongly convex.
	\end{enumerate}
\end{proposition}
%\end{restatable}
Condition \eqref{sc:l2}, which is actually the definition of \citet{Pol96} for strongly convex sets, means that $\cW$ can be obtained as the intersection of closed balls of radius $1/\lambda$, such that there is one ball for every boundary point $w$ and  tangent hyperplane $P$ where the ball touches $P$ in $w$. Note that a ball with radius $1/\lambda$ satisfies all conditions: \eqref{sc:l1} and \eqref{sc:l2} by definition, while \eqref{sc:l3} holds, e.g., by Example~13 of \citet{JourneeNRS10}.


\subsubsection{Regret bounds}


As promised, our next result connects the principal curvatures of $\bd(\cW)$ to the regret of FTL
and shows that FTL enjoys logarithmic regret for highly curved surfaces, as long as $\norm{\Theta_t}_2$ 
is bounded away from zero.
\begin{theorem}
\label{thm:R_curvesurface}
Assume $d \ge 2$ and let $\cW\subset \R^d$ be a convex body with twice continuously differentiable boundary.
Let $M = \max_{f\in \cF} \norm{f}_2$ and assume that $\Phi$ is differentiable at $(\Theta_t)_{t}$.
Assume that the principal curvatures of the surface $\bd(\cW)$ 
are all at least $\lambda_0$ for some constant $\lambda_0>0$ (that is, $\cW$ is $\lambda_0$ strongly convex) and $L_n:=\min_{1\le t \le n} \|\Theta_t\|_2 >0$. 
Choose $w_1\in \bd(\cW)$.
Then
\[
R_n \le \frac{2M^2}{\lambda_0 L_n}(1+ \log n)\,.
\]
\end{theorem}


%\begin{wrapfigure}{r}{0.3\textwidth}
%	\vspace{-0.8cm}
%\begin{framed}
%	\centering
%	\includegraphics[width=4.5cm, trim={4.5cm 1cm 3cm 0},clip]{figures/GaussmapPro}
%	\caption{Illustration of the construction used in the proof of~\eqref{eq:middletheta}.} 
%  	\label{fig:cuttingplane}
%\end{framed}
%\vspace{-.8cm}
%\end{wrapfigure} 
%
%
%

Before presenting the proof of the theorem, we discuss some of its implications and refinements. After the proof we will provide some examples of constraint sets with positive minimum principal curvature.

\begin{remark} \em
As we will show later in an essentially matching lower bound, this bound is tight, showing that the forte  %%%fort\'e???
of FTL is when $L_n$ %:=\min_{1\le t\le n} \norm{\Theta_t}_2$ 
is bounded away from zero and $\lambda_0$ is large.
Note that the bound is vacuous as soon as $L_n =O( \log n/n )$
and is worse than the minimax bound of $O(\sqrt{n})$ when $L _n = o( \log n/\sqrt{n} )$. 
One possibility to reduce the bound's sensitivity to $L_n$  
is to use the trivial bound $\ip{w_{t+1}-w_t,\Theta_t} \le L W = L \sup_{w,w'\in \cW} \norm{w-w'}_2$ 
for indices $t$ when $\norm{\Theta_t}_2\le L$ (with an arbitrary $L>0$). Then, by optimizing the bound over $L$, 
one gets a data-dependent bound
of the form 
\begin{equation}
\label{eq:Lbound}
R_n \le \inf_{L>0} \left(\frac{2M^2}{\lambda_0 L} (1+\log n) +  LW \, \sum_{t=1}^n t \,\one{ \norm{\Theta_t}_2\le L }\right),
\end{equation}
which is more complex, but is free of $L_n$ and thus reflects the nature of FTL better.
Note that in the case of stochastic problems, where $f_1,\ldots,f_n$ are independent and identically distributed (i.i.d.) with $\mu := -\Exp{\Theta_t}\ne 0$, the probability that $\norm{\Theta_t}_2 < \norm{\mu}_2/2$ is exponentially small in $t$. Thus, selecting $L=\norm{\mu}_2/2$ in the previous bound, the contribution of the expectation of the second term is $O(\norm{\mu}_2W)$, giving an overall bound of the form $O(\frac{M^2}{\lambda_0 \norm{\mu}_2}\log n+\norm{\mu}_2 W)$. 
On the other hand, if $\|\Theta_t\|_2 = 1$ if $t$ is odd and $\|\Theta_t\| = 0$ otherwise (such an example is trivial to construct), the optimal choice of $L$ in the above bound is $L= \Theta\left(\frac{M}{n}\sqrt{\frac{\log n}{\lambda_0 W}}\right)$ leads to a vacuous $O\left(Mn\sqrt{\frac{W\log n}{\lambda_0}}\right)$ regret bound.
\end{remark}
\begin{remark} \label{rem:iid} \em Now consider the case of i.i.d. losses in a bit more detail. Assume that $\cW=\cF=B_1$, the Euclidean unit ball centered at the origin, and assume $\E[f_t]=\mu \neq 0$.
Then it is straightforward to derive a high probability lower bound for $\|\Theta_t\|$: Using that $\E{\|\Theta_t\|_2^2} = \|\mu\|_2^2 + \frac{\sigma^2}{t}$ where $\sigma^2=\E{\inpro{f_i}{f_i}}-\|\mu\|_2^2$, we get
\begin{align*}
\Prob{\|\Theta_t\|_2 \le \frac{\|\mu\|_2}{2}} & %= \Prob{\|\Theta_t\|^2_2 \le \frac{\|\mu\|_2^2}{4}}
= \Prob{ \|\Theta_t\|^2_2 - \E{\|\Theta_t\|^2_2} \le -\frac{3\|\mu\|_2^2}{4} - \frac{\sigma^2}{t}}
\le e^{-\frac{t}{18}\left(\frac{3\|\mu\|_2^2}{4} + \frac{\sigma^2}{t}\right)^2} \le e^{-t\frac{\|\mu\|_2^4}{32}}~,
\end{align*}
where the first inequality is due to McDiarmid's inequality \citep{BoLuMa13} after noticing that changing a single $f_i$ to some $f'_i \in \cF$ may change the value of $\|\Theta_t\|_2^2$ by at most $6/t$.
Combining this with \eqref{eq:Lbound} for $L=\|\mu\|_2/2$, we get
\begin{equation}\label{eq:iidbound}
\E{R_n} \le \frac{4}{\|\mu\|_2} (1 + \log n) + \frac{\|\mu\|_2}{4 \sinh\left(\frac{\|\mu\|_2^4}{32}\right)} 
\le  \frac{4}{\|\mu\|_2} (1 + \log n) + O(1/\|\mu\|_2^7) % \frac{e^{\frac{\|\mu\|_2^4}{32}}}{\left(e^{\frac{\|\mu\|_2^4}{32}}-1\right)^2}
\end{equation}
\citet{koolen2016combiningA} also proved an  $O(\log n )$  bound on the expected regret of the sophisticated algorithm MetaGrad for the above case (Theorem~3 and Lemma~5 of their paper%\citealp{koolen2016combiningA}
): in particular, they showed that MetaGrad achieves $O(Bd\log n)$ regret where $B = \frac{2\lambda_{\max}}{\|\mu\|}$ with $\lambda_{\max} $ being the maximum eigenvalue of $\E[f_tf_t^{\top}]$.
Since $\lambda_{\max}$ in general can be as large as $1$ (if $\|f_t\|_2=1$), \eqref{eq:iidbound} can improve (asymptotically) a factor of $d$ over this regret bound. On the other hand, if 
$f_t$ is uniformly distributed on the half unit sphere (e.g., the first coordinate of $f_t$ is nonnegative with probability $1$), \citet{koolen2016combiningA} shows that $B\le \frac{24}{\sqrt{d}}$, which leads to an$O(\sqrt{d}\log n)$ regret, essentially matching \eqref{eq:iidbound}, as one can show that $\frac{c}{\sqrt{d}}\le \|\mu\|_2 \le \frac{1}{\sqrt{d}}$ for some constant $c$.
\end{remark}





\bigskip

\begin{proofof}{\cref{thm:R_curvesurface}}
Fix  $\theta_1, \theta_2 \in \real^d$ and let 
$w^{(1)} = \argmax_{w\in\cW}\inpro{w}{\theta_1}$,
$w^{(2)} = \argmax_{w\in\cW}\inpro{w}{\theta_2}$. 
Note that if $\theta_1,\theta_2\ne 0$ then $w^{(1)} , w^{(2)}  \in \bd(\cW)$. 
Below we will show that
\begin{align*}
\inpro{w^{(1)} - w^{(2)} }{\theta_1} 
	& \le \frac{1}{2\lambda_0} \frac{\|\theta_2 - \theta_1\|_2^2}{\|\theta_2\|_2}\,.
	 \numberthis\label{eq:middletheta}
\end{align*}
\cref{prop:regretabel}  suggests that it suffices to bound $\ip{w_{t+1}-w_t,\Theta_t}$. By \eqref{eq:middletheta}, we see
 that it suffices to bound how much $\Theta_t$ moves.
A straightforward calculation shows that $\Theta_t$ cannot move much:
%\begin{lemma}
for any norm $\norm{\cdot}$ on $\cF$, we have 
\begin{align}
\|\Theta_t - \Theta_{t-1} \| & = \left\|\frac{1}{t-1}\sum_{i=1}^{t-1} f_i - \frac{1}{t}\sum_{i=1}^{t} f_i \right\| 
	 = \left\| \sum_{i=1}^{t-1} \left( \frac{1}{t-1} - \frac{1}{t}\right) f_i- \frac{1}{t}f_t\right\| \nonumber \\
	& \le \left\| \sum_{i=1}^{t-1} \left( \frac{1}{t-1} - \frac{1}{t}\right) f_i \right\| + \left\| \frac{1}{t}f_t\right\| 
	 = \left\| \sum_{i=1}^{t-1} \frac{1}{t(t-1)} f_i \right\| + \left\| \frac{1}{t}f_t\right\| \nonumber \\
	 & = \frac{1}{t} \left\| \frac{1}{t-1} \sum_{i=1}^{t-1} f_i\right\| + \frac{1}{t}\left\|f_t\right\| 
	 \le \frac{2}{t}M\,. \label{prop:avgdiff}
\end{align}
where $M = \max_{f\in\cF} \|f\|$ is a constant that depends on $\cF$ and the norm $\norm{\cdot}$.
%\end{lemma}

Combining inequality \eqref{eq:middletheta} with \cref{prop:regretabel} and \eqref{prop:avgdiff}, we get
\begin{align*}
R_n &= \sum_{t=1}^{n} t\ip{ w_{t+1}-w_t,\Theta_t} %\\ & 
\le \sum_{t=1}^{n} \frac{t}{2\lambda_0} \frac{\|\Theta_t - \Theta_{t-1}\|_2^2}{\|\Theta_{t-1}\|_2} \\
&\le \frac{2M^2}{\lambda_0}\sum_{t=1}^{n} \frac{1}{t\|\Theta_{t-1}\|_2} \le \frac{2M^2}{\lambda_0L_n} \sum_{t=1}^{n} \frac{1}{t}
\le \frac{2M^2}{\lambda_0L_n} (1+\log n)\,.
\end{align*}

To finish the proof, it thus remains to show~\eqref{eq:middletheta}. Below we provide a derivation based on the definition of principal curvature.
Using the equivalence between the principal curvature and the modulus of strong convexity (cf., \cref{prop:strongconvex}), we also provide an alternative proof in \cref{app:stronglyconvexity} based on strong convexity, which leads to the slightly weaker result \eqref{middletheta2}, loosing a constant factor of 4. %\footnote{We thank the reviewer for this alternative proof.} 


The following elementary lemma relates the cosine of the angle between two vectors $\theta_1$ and $\theta_2$ to the squared normalized distance between 
the two vectors, thereby reducing our problem to bounding the cosine of this angle.
For brevity, we denote by $\cos\inangle{\theta_1}{\theta_2}$
the cosine of the angle between $\theta_1$ and $\theta_2$. 
\begin{lemma}
\label{lem:upperboundcos}
For any non-zero vectors $\theta_1, \theta_2 \in \real^d$,
\begin{align}
1- \cos \inangle{\theta_1}{\theta_2} \le \frac{1}{2} \frac{\|\theta_1 - \theta_2\|_2^2}{\|\theta_1\|_2\|\theta_2\|_2}.
\label{eq:angleineq}
\end{align}
\end{lemma}
\begin{proof}
Note that $\|\theta_1\|_2\|\theta_2\|_2\cos\inangle{\theta_1}{\theta_2} = \inpro{\theta_1}{\theta_2}$.
Therefore, \eqref{eq:angleineq} is equivalent to 
$ 2\|\theta_1\|_2\|\theta_2\|_2 - 2\inpro{\theta_1}{\theta_2} \le \|\theta_1 - \theta_2\|_2^2 $,
which, by algebraic manipulations, is itself equivalent to $0 \le (\|\theta_1\|_2-\|\theta_2\|_2)^2$.
\end{proof}


%\begin{wrapfigure}{r}{0.3\textwidth}
%	\vspace{-0.8cm}
%\begin{framed}
%	\centering
%	\includegraphics[width=4.5cm, trim={4.5cm 1cm 3cm 0},clip]{figures/GaussmapPro}
%	\caption{Illustration of the construction used in the proof of~\eqref{eq:middletheta}.} 
%  	\label{fig:cuttingplane}
%\end{framed}
%\vspace{-.8cm}
%\end{wrapfigure} 

\begin{figure}[h]
  \centering
	\includegraphics[height=4cm]{figures/GaussmapPro}
  \caption{Illustration of the construction used in the proof of~\eqref{eq:middletheta}.} 
  \label{fig:cuttingplane}
\end{figure}


Given this result, it suffices to show that $\cos \inangle{\theta_1}{\theta_2} \le 1-\lambda_0 \inpro{w^{(1)}-w^{(2)}}{\frac{\theta_1}{\|\theta_1\|_2}}$.
We prove this inequality using the differential geometry tools introduced introduced in \cref{sec:curvature_intro}.
Let $\ttheta_i = \frac{\theta_i}{\|\theta_i\|_2}$ for $i=1,2$.
The angle between $\theta_1$ and $\theta_2$ is the same as the angle between 
the normalized vectors $\ttheta_1$ and $\ttheta_2$.
To calculate the cosine of the angle between $\ttheta_1$ and $\ttheta_2$,
let $P$ be a plane spanned by $\ttheta_1$ and $w^{(1)}-w^{(2)}$ and passing through $w^{(1)}$
($P$ is uniquely determined if $\ttheta_1$ is not parallel to $w^{(1)}-w^{(2)}$;
if there are multiple planes, just pick any of them). 
Further, let $\htheta_2\in \bS^{d-1}$ be the unit vector along the projection of $\ttheta_2$ onto the plane $P$, as indicated in \cref{fig:cuttingplane}.
Clearly, $\cos \inangle{\ttheta_1}{\ttheta_2} \le \cos \inangle{\ttheta_1}{\htheta_2}$.


Consider a curve $\gamma(s)$ on $\bd(\cW)$ connecting $w^{(1)}$ and $w^{(2)}$ that is defined by the intersection of $\bd(\cW)$ and $P$ and is parametrized by its curve length $s$ so that $\gamma(0) = w^{(1)}$ and $\gamma(l) = w^{(2)}$, where $l$ is the length of the curve $\gamma$ between $w^{(1)}$ and $w^{(2)}$.
Note that since $\gamma$ is parametrized by its length, $\|\gamma'(s)\|_2 = 1$ for all $s \in [0,l]$. 
Let $u_{\cW}(w)$ denote the outer normal vector to $\cW$ at $w$ as before,
and let $u_\gamma\, : \, [0,l]\rightarrow \bS^{d-1}$ denote the Gauss map of the planar curve $\gamma$, that is, $u_\gamma(s) = \htheta$ where $\htheta$ is the unit vector parallel to the projection of $u_{\cW}(\gamma(s))$ on the plane $P$. 
%Thus $u_{\gamma}(s)$ is the Gauss map of the planar curve $\gamma$, and the curvature of $\gamma$ at $s$ is $\lambda(s)=\|u_{\gamma}'(s)\|_2$.
Now, for any $\theta \in \bS^{d-1}$, $w_\theta=\argmax_{w \in \cW} \inpro{w}{\theta}$ is a point where a hyperplane with normal vector $\theta$ touches $\cW$, thus, $u_{\cW}(w_\theta)=\theta$.
Therefore, $u_\gamma(0) = \ttheta_1$ and $u_\gamma(l) = \htheta_2$.
In fact $\gamma$ exists in two versions since $\cW$ is a compact convex body,
hence the intersection of $P$ and $\bd(\cW)$ is a closed curve.
Of these two versions we choose the one that satisfies that $\ip{\gamma'(s),\ttheta_1}\le 0$ for $s\in [0,l]$.\footnote{$\gamma'$ and $u'_\gamma$ denote the derivatives of $\gamma$ and $u_\gamma$, respectively, which exist since $\bd(\cW)$ is twice continuously differentiable. When $s=0$ or $s=l$, it suffices to take the corresponding one-sided derivatives or, equivalently, extend the definitions of $\gamma$ and $u_\gamma$ to an interval $[-\epsilon,l+\epsilon]$ for some $\epsilon>0$.}
Given the above, we have
\begin{align*}
\cos \inangle{\ttheta_1}{\htheta_2} & = \inpro{\htheta_2}{\ttheta_1} 
	 = 1 \! + \inpro{\htheta_2 - \ttheta_1}{\ttheta_1} 
	  = 1\!+ \Big\langle\int_{0}^{l} u_\gamma'(s)\,\text{d}s, \ttheta_1 \Big\rangle
	 = 1\!+ \!\int_{0}^{l} \inpro{u_\gamma'(s)}{\ttheta_1} \,\text{d}s. \numberthis \label{eq:cosint}
\end{align*}
Note that $\gamma$ is a planar curve on $\bd(\cW)$, 
thus its curvature $\lambda(s)$ satisfies $\lambda(s) \ge \lambda_0$ for any $s\in [0,l]$.
Also, for all $s \in [0,l]$, $\gamma'(s)$ is a unit vector parallel to $P$ (since $\gamma$ is parametrized by its curve length). 
Moreover, $u_\gamma'(s)$ is parallel to $\gamma'(s)$ since $u_{\gamma}(s)$ is the Gauss map, and $\lambda(s) = \|u_\gamma'(s)\|_2$.
Therefore, 
\[
\inpro{u_\gamma'(s)}{\ttheta_1} = \|u_\gamma'(s)\|_2\inpro{\gamma'(s)}{\ttheta_1} \le \lambda_0\inpro{\gamma'(s)}{\ttheta_1},
\]
where the last inequality holds because $\inpro{\gamma'(s)}{\ttheta_1} \le 0$.
Plugging this into~\eqref{eq:cosint}, we get the desired
\begin{align*}
\cos \inangle{\ttheta_1}{\htheta_2}   
	& \le 1+ \lambda_0\, \int_{0}^{l} \, \inpro{\gamma'(s)}{\ttheta_1}  \,\text{d}s 
	    = 1+ \lambda_0 \Big\langle\int_{0}^{l} \gamma'(s) \,\text{d}s, \ttheta_1 \Big\rangle
	  = 1 - \lambda_0 \inpro{w^{(1)}  - w^{(2)}}{\ttheta_1}\,.
\end{align*}
Reordering and combining with~\eqref{eq:angleineq} we obtain
\begin{align*}
\inpro{w^{(1)}  - w^{(2)}}{\ttheta_1} 
	& \le \frac{1}{\lambda_0} \left( 1- \cos \inangle{\ttheta_1}{\htheta_2} \right)
	%									  \left( 1- \cos \inpro{\ttheta_1}{\ttheta_2} \right) 
	 \le \frac{1}{\lambda_0} \left( 1- \cos \inangle{\theta_1}{\theta_2} \right) 
	 \le \frac{1}{2\lambda_0} \frac{\|\theta_1 - \theta_2\|_2^2}{\|\theta_1\|_2\|\theta_2\|_2}\,.
	% \numberthis\label{eq:middletheta},
\end{align*}
Multiplying both sides by $\norm{\theta_1}_2$ gives~\eqref{eq:middletheta}, thus, finishing the proof.
\end{proofof}

Next we present the smallest principal curvature of some common convex  bodies (the proofs are relegated to the appendix), often used as constraint sets in machine learning.

\begin{example}
	\label{ex:curvature}
\begin{enumerate}[(i)]\setlength{\itemsep}{0pt}
\item \label{it:ex1r} The smallest principal curvature $\lambda_0$ of the Euclidean ball $\cW = \set{w}{\|w\|_2\le r}$ of radius $r$ 
satisfies $\lambda_0=\frac{1}{r}$.

\item \label{it:ex1Q} Let $Q$ be a positive definite matrix.
If $\cW = \set{w}{w^\top Q w\le 1 }$ then $\lambda_0=\lambda_{\min}/\sqrt{\lambda_{\max}}$, 
where $\lambda_{\min}$ and $\lambda_{\max}$ are the minimal, respectively, maximal eigenvalues of $Q$.
%\item If $\cW$ is a cylinder, then $\lambda_0 = 0$, and \cref{thm:R_curvesurface} does not cover this case.
%However, from the proof, it is apparent that if $\Theta_t$ never enters the flat regions, we can still get a result
%where the definition of $\lambda_0$ is restricted to the ``curved part'' of $\bd(\cW)$. More generally, a local
%version of the theorem could also be easily given (for brevity, this result is omitted).

\item \label{it:ex1Lp} Let $p>1$ and $\cW = \seto{w\,\vert \|w\|_p\le1 }$. If $p>2$, then $\lambda_0 = 0$. Otherwise, if $1<p\le 2$, then 
$$\lambda_0 = \min_{w\in \bd(\cW)} \min_{v \in \bS^{d-1}:  \inpro{w^{\odot(p-1)}}{v}=0} (p-1)\frac{v^{\top} \diag\left(|w_1|^{p-2},\cdots,|w_d|^{p-2}\right) v }{\|w^{\odot(p-1)}\|_2}  \ge (p-1)d^{\frac{1}{2} - \frac{1}{p}}\,,$$
where $w^{\odot(p-1)}=(|w_1|^{p-1},\ldots,|w_d|^{p-1})$ and $\diag(a_1,\ldots,a_k)$ denotes a $k\times k$ diagonal matrix with diagonal entries $a_1,\ldots,a_k$.

\item \label{it:ex1gen} In general, let $\phi:\R^d \to \R$ be a twice continuously differentiable convex function.
Then, for $\cW = \set{w}{\phi(w)\le 1}$, 
$\lambda_0=\min_{w\in\bd(\cW)}\min_{v \in \bS^{d-1}:  \inpro{\phi'(w)}{v}=0} \frac{v^{\top}\nabla^2\phi(w) v}{\|\phi'(w)\|_2}$~. 
\end{enumerate}
\end{example}

Some of the results above have been derived in the literature based on seemingly different but equivalent assumptions explored in \cref{prop:strongconvex}:
 \eqref{it:ex1r} is a standard result in books on differential geometry; \citet{Pol96} derived \eqref{it:ex1Q} based on the strong convexity definition \eqref{sc:l2} in  \cref{prop:strongconvex}, while \eqref{it:ex1Lp} was proved by \citet{garber2014faster} based on the strong convexity definition \eqref{sc:l3} in  \cref{prop:strongconvex}. Other examples of strongly sets convex sets, that is, sets with positive minimal principal curvature, can be found in the paper of \citet{garber2014faster}.
 



%\subsection{$\alpha$-strong convexity and the smallest principal curvature of $\cW$}

Our last result in this section is a lower bound for the linear game, showing that FTL achieves the optimal rate under the condition that  $\min_t \|\Theta_t\|_2\ge L >0$.
\begin{theorem}
	\label{thm:lowerbound}
		Let $\lambda,L \in (0,1)$. Assume that  $\seto{(1,-L), (-1, -L)} \subset \cF$ and let \[
		\cW = \seto{(x,y) \in \R^2: x^2 + \frac{y^2}{\lambda^2} \le 1}
		\]
		be
		an ellipsoid with principal curvature $\lambda$.
 		Then, for any learning strategy, there exists a sequence of losses in $\mathcal F$ such that $\|\Theta_t\|_2 \ge L$ for all $t$ and
		\begin{align}\label{eq:Rnlb}
		R_n &\ge \frac{1}{84\sqrt{2}}\frac{1}{\lambda L} \log n  - \frac{1}{\lambda L} \left(\frac{2}{1-e^{-\lambda^2L^2}} + \frac{\pi^2}{108}\right)~.
		\end{align}
\end{theorem}
The theorem states that the regret of any learning strategy can be made at least  as large as $\Omega\left(\log n/(L\lambda)\right)$.
Note that by Example~\ref{ex:curvature}, the minimal principal curvature of $\cW$ in the above theorem is $\lambda$. In fact, it is not too hard to extend the above argument for any set $\cW$ such that there is $w \in \bd(\cW)$ where the curvature is $\lambda$, and the curvature is a continuous function in a neighborhood of $w$ over the boundary $\bd(\cW)$. The constants in the bound then depend on how fast the curvature changes within this neighborhood. In the case above, for small $\lambda L$, the $n$-independent term in \eqref{eq:Rnlb} is of order $1/(\lambda L)^3$.

\begin{proof}
 We define a random loss sequence, and we will show that no algorithm on this sequence can achieve an $o(\log n/ (\lambda_0 L)$ regret.
	Let $P$ be a random variable with $\mbox{Beta}(K,K)$ distribution for some $K>0$, and, given $P$, assume that $X_t, t \ge 1$ are i.i.d. Bernoulli random variables with parameter $P$. Let $f_t = X_t (1, -L) + (1-X_t) (-1, -L) = (2X_t - 1, -L)$. Thus, the second coordinate of $f_t$ is always $-L$, and so $\|\Theta_t\|_2 = \left\| \tfrac{1}{t} \sum_{i=1}^t f_i \right\|_2 \ge L$. Furthermore, the conditional expectation of the loss vector is $f^p \overset{\triangle}{=} \Expc{f_t}{P=p} = (2p - 1, -L)$. 
	
	Note that $X_t$ is a function of $f_t$ for all $t$; thus the conditional expectation of $P$, given $f_1,\ldots,f_{t-1}$, can be determined by the well-known formula $\hP_{t-1}= \Expc{P}{f_1 \ldots f_{t-1}} = \frac{K+\sum_{i=1}^{t-1} X_i}{2K+t-1}$.
	Given $p$, denote the optimizer of $f^p$ by $w^p$, that is, $w^p = \argmin_{w \in \cW} \inner{w,f^p}$. 	
	Then the Bayesian optimal choice in round $t$ is 
	\begin{align}
	\argmin_{w \in \mathcal W} \Expc{[\inner{w, f^P}}{ f_1\ldots f_{t-1}}
	&= \argmin_{w \in \mathcal W} \inner{w, \Expc{f^P}{f_1 \ldots f_{t-1}}} \nonumber \\
	&= \argmin_{w \in \mathcal W} \inner{w, f^{\hat P_{t-1}}} \nonumber \\
	&= w^{\hat P_{t-1}}\,,
	\label{eq:bayes-opt}
	\end{align}
	where the first equality follows by linearity of the inner product, the second since $f^p$ is a linear function of $p$ and the third
	by the definition of $w^p$.
	
	Thus, denoting by $W_t$ the prediction of an arbitrary algorithm in round $t$, the expected regret can be bounded from below as 
	\begin{align}
	\Exp{R_n}
	&= \Exp{\max_{w \in \cW} \sum_{t=1}^n \inner{W_t - w, f_t}}
	= \Exp{ \Expc{\max_{w \in \cW} \sum_{t=1}^n \inner{W_t - w, f_t}}{P} } \nonumber \\
	& \ge  \Exp{ \Expc{ \sum_{t=1}^n \inner{W_t - w^P, f_t}}{P} } = \Exp{\sum_{t=1}^n  \Expc{ \inner{W_t - w^P, f_t} }{P, f_1,\ldots,f_{t-1}}} \nonumber \\
	& = \Exp{\sum_{t=1}^n  \Expc{ \inner{W_t - w^P, f^P} }{f_1,\ldots,f_{t-1}}} \label{eq:Wf-ind} \\
	& \ge \Exp{\sum_{t=1}^n  \min_{w \in \cW} \Expc{ \inner{w- w^P, f^P} }{f_1,\ldots,f_{t-1}}} \nonumber \\
	& = \Exp{\sum_{t=1}^n  \Expc{ \inner{w^{\hP_{t-1}}- w^P, f^P} }{f_1,\ldots,f_{t-1}}}  \label{eq:bayes1} \\
	& = \sum_{t=1}^n \Exp{\inner{w^{\hP_{t-1}} - w^P, f^P}} \,, \nonumber 
	\end{align}
	where   \eqref{eq:Wf-ind} holds because of the independence of the $f_s$ given $P$ and since $W_t$ is chosen based on $f_1,\ldots,f_{t-1}$ (but not on $P$),
	%and $f_t$ are independent given $P,f_1,\ldots,f_{t-1}$ and $W_t$ is independent of $P$ given $f_1,\ldots,f_{t-1}$ and $f_t$ is independent of $f_1,\ldots,f_{t-1}$ given $P$ 
	and \eqref{eq:bayes1} holds by \eqref{eq:bayes-opt}. 
	
	By \cref{lem:P2P1loss}, given in \cref{sec:pr-lowerbound}, we have
	\begin{align}
	\sum_{t=1}^n \Exp{\inner{w^{\hP_{t-1}} - w^P, f^P}} 
	& \ge \frac{\lambda L}{2}\sum_{t=1}^n \Exp{ \frac{\left( \frac{2\hP_{t-1} - 2P}{\lambda L} \right)^2}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 } \left(1+\left( \frac{1-2\hP_{t-1}}{\lambda L}\right)^2 \right)} } \label{eq:hLloss} \\
	& = \frac{2}{\lambda L}\sum_{t=1}^n \Exp{\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\Expc{ \frac{ ( \hP_{t-1} - P)^2}{ 1+\left( \frac{1-2\hat P_{t-1}}{\lambda L}\right)^2 } }{P} } \nonumber \\
	& \ge \frac{2}{\lambda L}\sum_{t=1}^n \Exp{ \frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\Expc{ \frac{( \hP_{t-1} - P)^2}{ 1+ 2\left( \frac{1-2P}{\lambda L}\right)^2 +2 \left(\frac{2P - 2\hP_{t-1}}{\lambda L}\right)^2 }}{P } } \label{eq:hLlossCond}\,,
	\end{align}
	where in the last step we used $(a+b)^2 \le a^2 + b^2$. 	
	Let $\cG_t$ be the event that $|\hat P_{t} - P| \le \frac{K |1-2P|}{2K+t} + \frac{t \lambda L}{2K+t}$; note that $\cG_t$ holds with high probability by \cref{lem:concenPhat} in \cref{sec:pr-lowerbound}. Then, lower bounding the first term by $0$, \eqref{eq:hLlossCond} can be lower bounded by 
	\begin{align*}
	&\frac{2}{\lambda L}\sum_{t=1}^{n-1} \Exp{ \frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\Expc{ \frac{( \hP_{t} - P)^2}{ 1+ 2\left( \frac{1-2P}{\lambda L}\right)^2 +2 \left(\frac{2P - 2\hP_{t}}{\lambda L}\right)^2 }\ind(\cG_t)}{P } } \\
	&\ge \frac{2}{\lambda L}\sum_{t=1}^{n-1} \Exp{ \frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{\Expc{ ( \hP_{t} - P )^2\ind(\cG_t) }{P}}{ \left(1+ 2\left( \frac{1-2P}{\lambda L}\right)^2 +2 \left(\frac{2K}{2K+t}\frac{|1-2P|}{\lambda L} + \frac{2t}{2K+t}\right)^2 \right)}  } \\
	& \ge \frac{2}{\lambda L}\sum_{t=1}^{n-1} \Exp{\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{\Expc{ ( \hP_{t} - P )^2\ind(\cG_t) }{P}}{ \left(9+ 4\left( \frac{1-2P}{\lambda L}\right)^2 +8 \frac{|1-2P|}{\lambda L} \right)}  }.
	\end{align*}
	Combining the above, and using $(\hP_{t} - P )^2 \le 1$ together with the upper bound on the probability of the event $\cG^c_t$, the complement of $\cG_t$, given in \cref{lem:concenPhat}, we get
	\begin{align}
	\Exp{R_n} & \ge 
	\frac{2}{\lambda L}\sum_{t=1}^{n-1} \Exp{\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{\Expc{ ( \hP_{t} - P )^2 }{P}-\Prob{\cG^c_t}}{ \left(9+ 4\left( \frac{1-2P}{\lambda L}\right)^2 +8 \frac{|1-2P|}{\lambda L} \right)}  } \nonumber \\
	& \ge \frac{2}{\lambda L}\sum_{t=1}^{n-1} \left( \Exp{\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{\Expc{ ( \hP_{t} - P )^2 }{P}}{ \left(9+ 4\left( \frac{1-2P}{\lambda L}\right)^2 +8 \frac{|1-2P|}{\lambda L} \right)}  } - e^{-(t-1)\lambda^2L^2} \right) \nonumber \\
	& \ge \frac{2}{\lambda L}\left(\sum_{t=1}^{n-1} \Exp{\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{\Expc{ ( \hP_{t} - P )^2 }{P}}{ \left(9+ 4\left( \frac{1-2P}{\lambda L}\right)^2 +8 \frac{|1-2P|}{\lambda L} \right)}  }  \; -  \frac{1}{1-e^{-\lambda^2L^2}} \right)\,. \label{eq:Rngc}
	\end{align} 
	Now, by \cref{lem:bayeserror}, given in \cref{sec:pr-lowerbound}, we have
	\begin{align*}
	\Expc{ ( \hP_{t} - P )^2 }{P} & = \frac{K^2(1-2P)^2}{(2K+t)^2} + \frac{tP(1-P)}{(2K+t)^2} \ge P(1-P) \left( \frac{1}{t} - \frac{2}{t(2K+t)} \right)~. 
	\end{align*}
	Combining this with \eqref{eq:Rngc} and introducing the constant
	\[
	C = \Exp{\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{P(1-P)}{ \left(9+ 4\left( \frac{1-2P}{\lambda L}\right)^2 +8 \frac{|1-2P|}{\lambda L} \right)}  } 
	\]
	we obtain, for any $K>0$,
%	ASYMPTOTIC BELOW
%	\begin{align}
%	\liminf_{n \to \infty} \frac{\Exp{R_n}}{\log n} & \ge \liminf_{n \to \infty} \frac{2}{\lambda L \log n}\left[ - \frac{1}{1-e^{-\lambda^2L^2}} + \sum_{t=1}^{n-1} C\left(\frac{1}{t} - \frac{2}{t(2K+t)} \right) \right]
%	 = \frac{2 C}{\lambda L}~.
%	\end{align}
	\begin{align}
	\Exp{R_n}& \ge \frac{2}{\lambda L}\left[ - \frac{1}{1-e^{-\lambda^2L^2}} + \sum_{t=1}^{n-1} C\left(\frac{1}{t} - \frac{2}{t(2K+t)} \right) \right] \\
	 & \ge  \frac{2 C}{\lambda L} \log n - \frac{1}{\lambda L} \left(\frac{2}{1-e^{-\lambda^2L^2}} + \frac{C \pi^2}{3}\right)~.
	\end{align}
	where we used $\sum_{t=1}^{n-1} \ge \int_1^n 1/t = \log n$ and $\sum_{t=1}^{n-1} 1/(t(2K+t)) \le \sum_{t=1}^\infty 1/t^2 = \pi^2/6$.
	It remains to calculate a constant lower bound for $C$ that is independent of $\lambda$ and $L$. Denote $\frac{|1-2P|}{\lambda L}$ by $Y$; then $0\le P(1-P) = \frac{1-Y^2\lambda^2L^2}{4}\le 1/4$. Define $\widehat{\cG}$ to be the event when 
	$|Y| \le 1$. Since $P$ has $\mbox{Beta}(K,K)$ distribution, $\Exp{P} = \frac{1}{2}$ and $\mbox{Var}(P) = \frac{1}{8K}$. Therefore, by Chebyshev's inequality,
	\begin{align*}
	\Prob{\widehat{\cG}^c} = \Prob{ \left| P-\frac{1}{2}\right| > \frac{\lambda L}{2} } \le \frac{1}{2 K \lambda^2 L^2}~.
	\end{align*}
Therefore,
	\begin{align*}
	C &= \Exp{\frac{1}{\sqrt{1+Y^2 }}\frac{1-Y^2\lambda^2L^2}{ 4 (9+ 4Y^2 +8 Y )}}
	%& \E \left[\frac{1}{\sqrt{1+\left( \frac{1-2P}{\lambda L}\right)^2 }}\frac{P(1-P)}{ \left[9+ 4\left( \frac{1-2P}{\lambda L}\right)^2 +8 \frac{|1-2P|}{\lambda L} \right]}  \right] 
	 \ge \Exp{\frac{1}{\sqrt{1+Y^2 }}\frac{1-Y^2\lambda^2L^2}{ 4 (9+ 4Y^2 +8 Y )} \ind(\widehat{\cG}) } \\
	& \ge \frac{1}{84\sqrt{2}}\Exp{(1-Y^2\lambda^2L^2)\ind(\widehat{\cG})} 
	 \ge \frac{1}{84\sqrt{2}} \left( \Exp{1-Y^2\lambda^2L^2} - \Prob{\widehat{\cG}^c}\right) \\
	& \ge \frac{1}{84\sqrt{2}} \left( 1- \Exp{(1-2P)^2} - \frac{1}{2K\lambda^2L^2}\right)
	= \frac{1}{84\sqrt{2}} \left( 1 - \frac{1}{2K} - \frac{1}{2K\lambda^2L^2} \right) \\
	& \ge \frac{1}{84\sqrt{2}} \cdot \frac{1}{2}
	%=  \frac{1}{84\sqrt{2}} \left(\frac{1}{2} - \frac{\lambda^2L^2}{2}\right).
	\end{align*}
	for any $K \ge 1+ \frac{1}{\lambda^2 L^2}$.
	Hence, 
	\[
	\Exp{R_n} \ge \frac{1}{84\sqrt{2}}\frac{1}{\lambda L} \log n  - \frac{1}{\lambda L} \left(\frac{2}{1-e^{-\lambda^2L^2}} + \frac{\pi^2}{108}\right),
	\]
	where we used the trivial upper bound $C \le 1/36$ (obtained by maximizing the argument of the expectation in $P$ in the definition of $C$ by selecting $P=1/2$).
	The result is completed by noting that the worst-case regret is at least as big as the expected regret, thus, for every $n$, there exist a $P$ and a sequence of loss vectors $f_1,\ldots,f_n$ such that the regret $R_n$ satisfies \eqref{eq:Rnlb}. %is at least $\Omega(\frac{\log n }{\lambda L})$.
\end{proof}


\subsection{Other regularities}

So far we have looked at the case when FTL achieves a low regret due to the curvature of $\bd(\cW)$.
The next result characterizes the regret of FTL when $\cW$ is a polytope, which has a flat, non-smooth boundary and thus \cref{thm:R_curvesurface} is not applicable. 
For this statement recall that given some norm $\|\cdot\|$,  its dual norm is defined by $\|w\|_* = \sup_{\|v\|\le 1} \inpro{v}{w}$.
\begin{theorem}
	\label{thm:regretpolytope}
	Assume that $\cW$ is a polytope
	and that $\Phi$ is differentiable at $\Theta_i$, $i= 1, \ldots, n$. 
	Let $w_t = \argmax_{w\in\cW} \inpro{w}{\Theta_{t-1}}$,
	$W = \sup_{w_1,w_2\in\cW}\|w_1 - w_2\|_*$ and $F = \sup_{f_1,f_2\in \cF} \norm{f_1-f_2}$.
	 Then the regret of FTL is 
	\[
	R_n \le W\, \sum_{t=1}^{n} t \,\ind(w_{t+1}\neq w_{t})  \|\Theta_t - \Theta_{t-1}\| \le FW\,\sum_{t=1}^{n} \ind(w_{t+1}\neq w_{t})\,.
	\]
\end{theorem}
Note that when $\cW$ is a polytope, $w_t$ is expected to ``snap'' to some vertex of $\cW$. Hence, 
we expect the regret bound to be non-vacuous, if, e.g., $\Theta_t$ ``stabilizes'' around some value. Some examples after the 
proof will illustrate this.
\begin{proof}
Let $v \!=\! \argmax_{w\in\cW} \inpro{w}{\theta}$, $v'\!=\!\argmax_{w\in \cW}\ip{w,\theta'}$. 
Similarly to the proof of \cref{thm:R_curvesurface},
	\begin{align*}
	\inpro{v'-v}{\theta'} & = \inpro{v'}{\theta'} - \inpro{v'}{\theta} + \inpro{v'}{\theta} - \inpro{v}{\theta} + \inpro{v}{\theta} -\inpro{v}{\theta'} \\
	& \le \inpro{v'}{\theta'} - \inpro{v'}{\theta} + \inpro{v}{\theta} -\inpro{v}{\theta'} %\numberthis \label{eq:eq211}
	= \inpro{v' - v}{\theta' - \theta} 
	%\\& 
	\le W\,\ind(v'\neq v)\|\theta' - \theta \|,
	\end{align*}
	where the first inequality %~\eqref{eq:eq211} 
	holds because $\inpro{v'}{\theta} \le \inpro{v}{\theta}$.
	Therefore, by \eqref{prop:avgdiff}, 
	\begin{align*}
	R_n & = \sum_{t=1}^n t\,\ip{ w_{t+1}-w_t,\Theta_t} 
	 \le W\,\sum_{t=1}^{n} t\, \ind(w_{t+1}\!\neq\! w_{t})  \|\Theta_t - \Theta_{t-1}\| 
	 \le FW\,\sum_{t=1}^{n} \ind(w_{t+1}\!\neq\! w_{t})\,.
	\end{align*}
\end{proof}
\if0
\begin{comm}
	\cref{thm:regretpolytope} bounds the regret of FTL by the number of switches of the maximizers $\sum_{t=1}^{n} \ind(w_t\neq w_{t-1})$.
\end{comm}
\fi
%\begin{comm}
As noted before,  since $\cW$ is a polytope, $w_t$ is (generally) attained at the vertices. 
In this case, the epigraph of $\Phi$  is a polyhedral cone. Then, the event when $w_{t+1}\neq w_{t}$, that is, when 
	the ``leader'' switches corresponds to when 
	$\Theta_{t}$ and $\Theta_{t-1}$ belong to different linear regions corresponding to different linear pieces of the graph of $\Phi$.
%\end{comm}

We now spell out a corollary for the stochastic setting. In particular, in this case FTL will often enjoy a constant regret:
\begin{corollary}[Stochastic setting]
	\label{cor:stocpolytope} Assume that $\cW$ is a polytope and 
	that $(f_t)_{1\le t \le n}$ is an i.i.d. sequence of random variables 
	such that $\Exp{f_i} = \mu$ and $\|f_i\|_\infty \le M$. Let  $W = \sup_{w_1,w_2\in \cW} \norm{w_1-w_2}_1$.
	Further assume that there exists a constant $r > 0$ 
	such that $\Phi$ is differentiable for any $\nu$ such that $\|\nu-\mu\|_\infty \le r$. 
	Then, % there exists a constant $C$, such that 
	\[
		\Exp{R_n} \le 2MW \, \left( 3 + \frac{2M^2}{r^2}\log\left(\frac{2M^2d}{r^2}\right)\right)\,.%2MW \, (1+4d M^2/r^2 )\,.
%	O(\sum_{t=1}^{n} \Prob{-\Theta_t \notin V}) = O(1).
	\]
\end{corollary}
 	The existence of an $r$ such that $\Phi$ is differentiable for any $\nu$ such that $\|\nu-\mu\|_\infty \le r$ is equivalent to that $\Phi$ is differentiable at $\mu$. 
	By \cref{prop:derivativePhi}, this condition requires that at $\mu$, $\max_{w\in\cW} \ip{w,\theta}$ has a unique optimizer (note that the volume of the set of vectors $\theta$ with multiple optimizers is zero). 
	On the other hand,  $r$ should be selected to be the radius of the largest ball such that the optimal decisions for the expected losses $\mu$ and $\nu$ (i.e., the maximizers defining $\Phi(-\mu)$ and $\Phi(-\nu)$) belong to the same face of $\cW$.
\begin{proof}
	Let $V = \set{\nu}{\|\nu - \mu\|_\infty\le r}$. 
	Note that the epigraph of the function $\Phi$ is a polyhedral cone.
	Since $\Phi$ is differentiable in the interior of $V$, $\set{(\theta, \Phi(\theta))}{\theta\in V}$ is a subset of a linear subspace.
	Therefore, for $-\Theta_t, -\Theta_{t-1} \in V$, $w_{t+1}=w_t$.
	Hence, by \cref{thm:regretpolytope},
	\begin{equation}
	\label{eq:sp1}
	\Exp{R_n} \le 2MW\,\sum_{t=1}^{n} \Prob{-\Theta_t,-\Theta_{t-1} \notin V}
	 \le 4MW\,\left(1+\sum_{t=1}^{n} \Prob{-\Theta_t \notin V}\right)\,.
	\end{equation}
	On the other hand, note that $\|f_i\|_\infty\le M$.
	Then 
	\begin{align*}
	\Prob{-\Theta_t \notin V}
	    & = \Prob{ \norm{\frac{1}{t} \sum_{i=1}^{t} f_i - \mu}_\infty \ge r}
%		& \le \Pr\left( \|\frac{1}{t} \sum_{i=1}^{t} f_i - \mu\|_\infty \ge \frac{r}{\sqrt{d}}\right) \\
		 \le \sum_{j=1}^{d} \Prob{ \left|\frac{1}{t} \sum_{i=1}^{t} f_{i,j} - \mu_j\right| \ge r }
%		& \le 2 \sum_{j=1}^{d} \text{exp}\left( -\frac{tr^2}{2M^2}\right) \numberthis \label{eq:eq212}\\
		 \le 2d e^{-\frac{tr^2}{2M^2}}\,,
	\end{align*}
	where the last inequality
	%~\eqref{eq:eq212} 
	is due to Hoeffding's inequality.
	Now, using that for any $\alpha>0$ and  $\tau > 0$, $\sum_{t=\tau+1}^n \exp(-\alpha t ) \le \int_\tau^n \exp(-\alpha t ) dt 
	\le \frac{1}{\alpha}\exp(-\alpha\tau)$, from \eqref{eq:sp1} we obtain
\[
	\Exp{R_n} \le 2MW \, \left(1+\tau + \frac{2d}{\alpha}e^{-\alpha\tau} \right)~.
\]
Setting $\alpha = \frac{r^2}{2M^2}$ and $\tau = \frac{1}{\alpha}\log(d/\alpha)$ in the above bound finishes the proof.
\end{proof}
\if0
\begin{comm}
	Existing results of FTL in the expert setting  when $\cW = \set{w}{w\ge 0, \|w\|_1\le 1}$ can be recovered from \cref{thm:regretpolytope} and \cref{cor:stocpolytope}, that FTL achieves constant regret in the stochastic setting, while it may suffer a linear regret in the adversarial setting.
\end{comm}
\fi
%\begin{comm}
%\end{comm}

\section{Adaptive algorithms} % for the linear game}
While as shown in \cref{thm:R_curvesurface}, FTL can exploit the curvature of the surface of the constraint set to achieve $O(\log n)$ regret, it requires the curvature condition and $\min_t \|\Theta_t\|_2 \ge L$ being bounded away from zero, or
 it may suffer even linear regret.
On the other hand, many algorithms,  such as the follow the regularized leader (FTRL) algorithm \citep[see,e.g.,][]{SS12:Book}, are known to achieve a regret guarantee of $O(\sqrt{n})$ even for the worst-case data in the linear setting.
This raises the question whether one can have an algorithm that can 
achieve constant or $O(\log n)$ regret in the respective settings of  \cref{cor:stocpolytope} or \cref{thm:R_curvesurface},
while it still maintains $O(\sqrt{n})$ regret for worst-case data. 
One way to design an adaptive algorithm is to use the $(\cA, \cB)$-prod algorithm of \citet{sani2014exploiting}, trivially leading to the following result:
\begin{proposition}
Consider $(\cA,\cB)$-prod of \citet{sani2014exploiting}, where algorithm \todoa{Do we need to write out $(\cA,\cB)$-prod?}
 $\cA$ is chosen to be FTRL with an appropriate regularization term, 
 while $\cB$ is chosen to be FTL. 
Then the regret of the resulting hybrid algorithm $\cH$ enjoys the following guarantees:
%(i) If FTL achieves constant regret as in the setting of \cref{cor:stocpolytope}, then the regret of $\cH$ is also constant.
%(ii) If FTL achieves a regret of $O(\log n)$ as in the setting of \cref{thm:R_curvesurface}, then the regret of $\cH$ is also $O(\log n)$.
%(iii) Otherwise, the regret of $\cH$ is at most $O(\sqrt{n\log n})$.
\begin{itemize}\setlength{\itemsep}{0pt}
\item If FTL achieves constant regret as in the setting of \cref{cor:stocpolytope}, then the regret of $\cH$ is also constant.
\item If FTL achieves a regret of $O(\log n)$ as in the setting of \cref{thm:R_curvesurface}, then the regret of $\cH$ is also $O(\log n)$.
\item Otherwise, the regret of $\cH$ is at most $O(\sqrt{n\log n})$.
\end{itemize}
\end{proposition} 

In the next section we show that if the constraint set is the unit ball, it is possible to design adaptive algorithms directly.

\subsection{Adaptive Algorithms for Ellipsoid Constraint Sets}

In this section we provide some interesting results about adaptive algorithms for the case when $\cW$ is an ellipsoid in $\R^d$. First, we show that a variant of FTL using shrinkage as regularization has $O(\log n)$ regret when $\|\Theta_t\|_2 \ge L>0$ for all $t$, but it also has $O(\sqrt{n})$ worst case guarantee. Furthermore, we show that the standard FTRL algorithm is adaptive if the constraint set is an ellipsoid and the loss vectors are stochastic.
Throughout the section we will use the notation $F_t=-(t-1)\Theta_t=\sum_{i=1}^{t-1} f_i$.

\subsubsection{Follow the Shrunken Leader}

In this section we are going to analyze a combination of the FTL algorithm and the idea of shrinkage often used for regularization purposes in statistics. We assume that $\cW$ is the $d$-dimensional unit ball and,  without loss of generality, we further assume that $\|f\|_2 \le 1$ for all $f\in\cF$. 
	\begin{algorithm}[t]
		\caption{Follow The Shrunken Leader (FTSL)}
		\label{alg:adaptiveAlgorithm}
		\begin{algorithmic}[1]
			\STATE Predict $w_1 = 0$; 
			\FOR {$t = 2, ..., n-1$}
			\STATE {FTL: Compute $\tilde{w}_{t} = \argmin_{w\in\cW} \inner{ w, F_{t-1}}$.}
			\STATE {Shrinkage: Predict $w_t = \frac{\|F_{t-1}\|_2}{\sqrt{\|F_{t-1}\|_2^2+t+2}}\tilde{w}_{t}$.}
			\ENDFOR
			\STATE {FTL: Compute $\tilde{w}_{n} = \argmin_{w\in\cW} \inner{ w, F_{n-1}}$.}
			\STATE {Shrinkage: Predict $w_n = \frac{\|F_{n-1}\|_2}{\sqrt{\|F_{n-1}\|_2^2+n}}\tilde{w}_{n}$.}
		\end{algorithmic}
	\end{algorithm}
 The Follow The Shrunken Leader (FTSL) algorithm is given in \cref{alg:adaptiveAlgorithm}. The main idea of the algorithm is to predict a shrunken version of the FTL prediction, in this way keeping it away from the boundary of $\cW$. The next theorem shows that the right amount of shrinkage leads to a robust, adaptive algorithm.
\begin{theorem}
	Assume that $\cW=\set{x \in \R^d}{ \|x\|_2 \le 1}$ and $\|f\|_2 \le 1$ for all $f\in\cF$. Then the regret of FTSL is $O(\sqrt{n})$.
If, in addition, there exists an $L>0$ such that $\|\Theta_t\|_2 \ge L$ for $1\le t\le n$, then the regret of is $O(\log n/L)$.
\end{theorem}
\begin{proof}
	By the definition of $F_t$ and $\cW$, $\tilde{w}_{t} =- F_{t-1}/\|F_{t-1}\|_2$.
	Let $\sigma_n = \frac{\|F_{n-1}\|_2}{\sqrt{\|F_{n-1}\|_2^2 + n}}$.
	Our proof follows the idea of \citet{abernethy2008optimal}. We compute the upper bound on the value of the game for each round backwards for $t=n,n-1,\dots,1$, by solving the optimal strategies for $f_t$.
	The value of the game using FTSL is defined as
	\begin{align*}
	V_n & = \max_{f_1, \ldots, f_n} \sum_{t=1}^{n}\inpro{w_t}{f_t}- \min_{w\in\cW} \inpro{w}{F_n} \\
	& = \max_{f_1,\ldots, f_{n-1}} \sum_{t=1}^{n-1}\inpro{w_t}{f_t} + \underbrace{\max_{f_n} \|F_{n-1}+f_n\|_2 + \inpro{f_n}{w_n}}_{=:U_n}
	\end{align*}
We first prove that $U_n$, the second term above, is bounded from above by  $\sqrt{\|F_{n-1}\|_2^2 + n}$. To see this, let $f_n = a_n \tilde{F}_{n-1} + b_n \Omega_{n-1}$ where $\tilde{F}_{n-1}$ is the unit vector parallel to $F_{n-1}$ and $\Omega_{n-1}$ is a unit vector orthogonal to $F_{n-1}$.  Furthermore, since $\|f_n\|_2 \le 1$, we have $a_n^2+b_n^2 \le 1$.
Thus,
\begin{align*}
U_n & = \max_{f_n} \sqrt{\|F_{n-1}\|_2^2 + 2a_n\|F_{n-1}\|_2 + a_n^2 + b_n^2} - a_n\sigma_{n}\\
 & \le \max_{a} \sqrt{\|F_{n-1}\|_2^2 + 2a\|F_{n-1}\|_2 + n} - a\sigma_{n}\\
 & = \sqrt{\|F_{n-1}\|_2^2 + n},
\end{align*}  
where the last equality follows since the maximum is attained at $a=0$. 
A similar statement holds for the other time indices: for any $t \ge 1$,
\begin{equation}
\label{eq:stepDiff1}
\max_{f_t} \sqrt{\|F_{t-1} + f_t\|_2^2 + t + 1} + \inpro{f_t}{w_t} \le  \sqrt{\|F_{t-1}\|_2^2 + t} + \frac{1}{\sqrt{t}}~.
\end{equation}
Before proving this inequality, we show how it implies the first statement of the theorem:
\begin{align*}
V_n & \le \max_{f_1,\ldots, f_{n-1}} \sum_{t=1}^{n-1}\inpro{w_t}{f_t} + \sqrt{\|F_{n-1}\|_2^2 + n} \\
& \le \max_{f_1,\ldots, f_{n-2}} \sum_{t=1}^{n-2}\inpro{w_t}{f_t}  + \sqrt{\|F_{n-2}\|_2^2 + n-1} + \frac{1}{\sqrt{n-1}} \\
& \le \ldots \\
& \le 1+ \sum_{t=1}^{n-1}\frac{1}{\sqrt{t}} \le 2(1+\sqrt{n-1}).
\end{align*}
Moreover, if $\|\Theta_t\|_2 \ge L>0$ for $1\le t\le n$, a stronger version of \eqref{eq:stepDiff1} also holds:
\begin{equation}
\label{eq:stepDiff2}
\max_{f_t} \sqrt{\|F_{t-1} + f_t\|_2^2 + t + 1} + \inpro{f_t}{w_t} \le  \sqrt{\|F_{t-1}\|_2^2 + t} + \frac{1}{(t-1)L}.
\end{equation}
This implies the second statement of the theorem, since
\begin{align*}
V_n & \le \max_{f_1,\ldots, f_{n-1}} \sum_{t=1}^{n-1}\inpro{w_t}{f_t} + \sqrt{\|F_{n-1}\|_2^2 + n} \\
& \le \max_{f_1,\ldots, f_{n-2}} \sum_{t=1}^{n-2}\inpro{w_t}{f_t}  + \sqrt{\|F_{n-2}\|_2^2 + n-1} + \frac{1}{(n-1)L} \\
& \le \ldots \\
& \le 1+ \sum_{t=1}^{n-1}\frac{1}{tL} \le 1+ \frac{1}{L} +\frac{\log(n-1)}{L}~.
\end{align*}

To finish the proof, it remains to show \eqref{eq:stepDiff1} and \eqref{eq:stepDiff2}.
Let  $f_t = a_t \tilde{F}_{t-1} + b_t \Omega_{t-1}$ where $\tilde{F}_{t-1}$ is the unit vector parallel to $F_{t-1}$ and $\Omega_{t-1}$ is a unit vector orthogonal to $F_{t-1}$. Since $\|f_t\|_2 \le 1$, observe that $a_t^2+b_t^2 =\|f_t\|_2 \le 1$. Furthermore, let $\sigma_t = \frac{\|F_{t-1}\|_2}{\sqrt{\|F_{t-1}\|_2^2 + t+2}}$.
Then, for any $t \ge 1$,
	\begin{align}
	\Delta_t & =\max_{f_t} \sqrt{\|F_{t-1}\|_2^2 + 2a_t\|F_{t-1}\|_2 + a_t^2 + b_t^2 + t+1} - a_t\sigma_t  - \sqrt{\|F_{t-1}\|_2^2 + t}  \nonumber\\
	& \le \max_{a_t} \sqrt{\|F_{t-1}\|_2^2 + 2a_t\|F_{t-1}\|_2 + t+2} - a_t\sigma_t  - \sqrt{\|F_{t-1}\|_2^2 + t}  \nonumber\\
	& = \sqrt{\|F_{t-1}\|_2^2 + t+2} - \sqrt{\|F_{t-1}\|_2^2 + t}  \nonumber\\
	& = \frac{2}{\sqrt{\|F_{t-1}\|_2^2 + t+2} + \sqrt{\|F_{t-1}\|_2^2 + t}} \label{eq:stepDiff3} \\
	& \le \frac{1}{\sqrt{t}}. \nonumber
	\end{align}
	This proves \eqref{eq:stepDiff1}.
	Moreover, if $\|F_{t-1}\|_2 = \|(t-1)\Theta\|_2 \ge (t-1)L>0$, by \eqref{eq:stepDiff3} we obtain
	\[
	\Delta_t \le \frac{2}{\sqrt{\|F_{t-1}\|_2^2 + t+2} + \sqrt{\|F_{t-1}\|_2^2 + t}} \le \frac{1}{\|F_{t-1}\|_2}\le \frac{1}{(t-1)L},
	\]
	proving \eqref{eq:stepDiff2}.
\end{proof}
\begin{remark} \em
	The above result can easily be extended to the case when $\cW$ is an ellipsoid, that is, $\cW = \seto{w \,\vert\, w^{\top}Qw \le 1}$ for some positive-definite matrix $Q$. Transforming the predictions as
	$\hat{w}_t = Q^{1/2}w_t$ and the losses $\hat{f}_t=Q^{-1/2} f_t$, we see that, for all $t$, the new prediction $\hat{w}_t$ belongs to the unit ball (i.e., $\hat{w}_t \in B_1$) and 
	$\inpro{w_t}{f_t}=\inpro{\hat{w}_t}{\hat{f}_t}$. Thus, the value of the game can be bounded as
	\begin{align*}
	V_n & = \max_{f_1, \ldots, f_n\,:\, \|f_i\|_2\le 1} \sum_{t=1}^{n}\inpro{w_t}{f_t}- \min_{w\in\cW} \inpro{w}{F_n} \\
	&  = \max_{f_1, \ldots, f_n\,:\, \|f_i\|_2\le 1} \sum_{t=1}^{n} \inpro{\hat{w}_t}{Q^{-1/2}f_t} - \min_{\hat{w}\in B_1} \inpro{\hat{w}}{Q^{-1/2}F_n} \\
	& =  \max_{\hat{f}_1, \ldots, \hat{f}_n\,:\, \|Q^{1/2}\hat{f}_i\|_2\le 1} \sum_{t=1}^{n} \inpro{\hat{w}_t}{\hat{f}_t} - \min_{\hat{w}\in B_1} \inpro{\hat{w}}{\widehat{F}_n} \\
	& \le \max_{\hat{f}_1, \ldots, \hat{f}_n\,:\, \|\hat{f}_i\|_2\le \frac{1}{\sqrt{\lambda_{\min}}}} \sum_{t=1}^{n} \inpro{\hat{w}_t}{\hat{f}_t} - \min_{\hat{w}\in B_1} \inpro{\hat{w}}{\widehat{F}_n} \\
	& = \frac{1}{\sqrt{\lambda_{\min}}} \max_{\tilde{f}_1, \ldots, \tilde{f}_n\,:\, \|\tilde{f}_i\|_2\le 1} \sum_{t=1}^{n} \inpro{\hat{w}_t}{\tilde{f}_t} - \min_{\hat{w}\in B_1} \inpro{\hat{w}}{\tilde{F}_n}\,,
	\end{align*}
	where $\lambda_{\min}$ is the minimal eigenvalue of the matrix $Q$, $\widehat{F}_n=\sum_{t=1}^n \hat{f}_t$, $\tilde{f}_t=\sqrt{\lambda_{\min} }\ \hat{f}_t$, and $\tilde{F}_n=\sqrt{\lambda_{\min} }\  \hat{F}_n=\sum_{t=1}^n \tilde{f}_t$. Thus, playing over $\cW$ with loss vectors from the unit ball is equivalent to playing over the unit ball $B_1$ against losses over $\cW$ (note that $\|Q^{1/2}\hat{f}_i\|_2\le 1$ is equivalent to $\hat{f}_i \in \cW$), which can be reduced to playing against losses with maximum Euclidean norm $1$. Thus, an algorithm for the ellipsoid constraint set $\cW$ is to run FTSL over the unit ball $B_1$ with the transformed losses $\tilde{f}_t$, and predict  $w_t=Q^{-1/2} \hat{w}_t$ where $\hat{w}_t$ is the prediction of FTSL.
	Assuming that $\| \Theta_t \|_2 \ge L$ for all $t$ in the original problem, in the transformed problem we have $\|\sqrt{\lambda_{\min}}\ Q^{-1/2} \Theta_t\|_2 \ge L \sqrt{\lambda_{\min}/\lambda_{\max}}$ where $\lambda_{\max}$ is the largest eigenvalue of $Q$. Hence, the regret of the algorithm (in both the original and the transformed problems) is at most $O\big(\frac{\sqrt{\lambda_{\max}}}{L \lambda_{\min}} \log n \big)$. Note that this is exactly the same rate as we can obtain from \cref{thm:R_curvesurface} and \cref{ex:curvature} \eqref{it:ex1Q} for the (non-adaptive) FTL algorithm for the ellipsoid constraint set $\cW$ (a closer inspection of the constants shows that the leading constant for FTSL is actually a factor of $2$ better).
\end{remark}


\subsubsection{FTRL for stochastic losses}
\label{subsubsec:FTRLunitball}
This section shows that when $\cW$ is the unit ball $B_1$, FTRL with regularizer $R(w) = \frac{1}{2}\|w\|^2$ is an adaptive algorithm achieving logarithmic regret for stochastic losses. To fix the notation, in round $t$, FTLR predicts
\[
	w_{t} = \argmin_{w\in \cW} \eta_t \inpro{F_{t-1}}{w} + R(w), 
\]
if $t >1$ and $w_1=0$.
It has been well known that FTRL with $\eta_t = 1/\sqrt{t-1}$ is guaranteed to achieve $O(\sqrt{n})$ regret in the adversarial setting, see, e.g., \citep{SS12:Book}. It remains to prove that FTRL indeed achieves a fast rate in the stochastic setting. 
\begin{theorem}
	Assume that the sequence of loss vectors, $f_1,\ldots,f_n \in \R^d$  satisfies $\|f_t\|_2 \le 1$ almost surely and $\Exp{f_t} = \mu$ for all $t$ with some $\|\mu\|_2 >0$. Then FTRL with $\eta_t=1/\sqrt{t-1}$ suffers $O(\log n)$ regret .
\end{theorem}

\begin{proof}
	Using $R(w) = \frac{1}{2}\|w\|^2$ as its regularization, in round $t>1$ FTRL predicts 
	\begin{equation}
	\label{eq:ftrl-eq}
	w_{t} = \argmin_{w\in \cW} \eta_t \inpro{F_{t-1}}{w} + R(w) = 
	\begin{cases}
	\frac{1}{\sqrt{t-1}} F_{t-1} & \quad \text{if  } \|F_{t-1}\| \le \sqrt{t-1} \\
	\frac{F_{t-1}}{\|F_{t-1}\|} & \quad \text{otherwise.} 
	\end{cases}
	\end{equation}
	For any $1\le t \le n$,  denote the event $\|F_t\| \ge \sqrt{t}$ by $\cE_t$. Note that if $\|F_{t-1}\| \ge \sqrt{t-1}$, FTRL predicts exactly the same $w_t$ as FTL. Denote the accumulated loss of FTL in $n$ rounds by $\cL^{FTL}_n$. Thus, the regret of FTRL is
	\begin{align*}
	\Exp{R_n} & = \Exp{\sum_{t=1}^{n} \inpro{f_t}{w_t} - \min_{w\in\cW}  \inpro{f_t}{w}} \\
	& = \Exp{ \sum_{t=1}^{n} \inpro{f_t}{w_t} - \cL^{FTL}_n }+ \Exp{\cL^{FTL}_n - \min_{w\in\cW} \inpro{f_t}{w} } \\
	& \le 2 \sum_{t=1}^{n} \Prob{\cE_t^c} + O(\log n),
	\end{align*}
	where, to obtain the last inequality, we applied \eqref{eq:ftrl-eq} for the first term, while the second term is $O(\log n)$ by \cref{rem:iid}. \todoa{This is somewhat imprecise, though...}
	It remains to bound the first term, 2 $\sum_{t=1}^{n} \Prob{\cE_t^c} $ in the above.
	For any $t > \frac{4}{\|\mu\|_2^2}$,
	\begin{align*}
	\Prob{ \|F_{t}\|_2 \le \sqrt{t} } &\le \Prob{ \|F_{t}\|_2 <  \frac{t}{2}\|\mu\|_2 } \le \sum_{i=1}^d \Prob{|F_{t,i}| < \frac{t}{2} |\mu_i|} \\
	             &\le \sum_{i=1}^d \Prob{|F_{t,i}-t\mu_i| >  \frac{t}{2} |\mu_i|} \le 2 \sum_{i=1}^d e^{-\frac{\mu_i^2}{4} t}
	\end{align*}
	Thus,
	\begin{align*}
	\sum_{t=1}^{n} \Prob{\cE_t^c}  & = \sum_{t=1}^{4/\|\mu\|_2^2} \Prob{\cE_t^c}  + \sum_{t=4/\|\mu\|_2^2}^{n} \Prob{\cE_t^c} \\
	& \le \frac{4}{\|\mu\|_2^2} + 2\sum_{i=1}^d \sum_{t=0}^{n} e^{-\frac{\mu_i^2}{4} t} \\
	& \le \frac{4}{\|\mu\|_2^2} + 2\sum_{i=1}^d \frac{1}{1-e^{ -\frac{\mu_i^2}{4}}} \\
	& \le  \frac{4}{\|\mu\|_2^2} + 2\sum_{i=1}^d \frac{\mu_i^2}{4} = \frac{4}{\|\mu\|_2^2}  + \frac{\|\mu\|_2^2}{2}~.
	\end{align*}
	where in the last inequality we used $1/(1-e^{-a}) \le a$.
	Therefore, if $\|\mu\| > 0$, the regret of FTRL satisfies
	\[
	\Exp{R_n} \le \frac{8}{\|\mu\|_2^2} + \|\mu\|_2^2 + O(\log n) = O(\log n).
	\]
\end{proof}

%{FTRL for  the case of the general eclipse constraint set}
\begin{remark}\em
Similarly to FTSL,  the above result can be extended to ellipsoid constraint sets with an adequate choice of the regularizer $R(w)$.
Assume that $\cW = \seto{ w \,\vert\, w^{\top}Qw \le 1}$ for some positive definite matrix $Q$, and let $R(w) = \frac{1}{2} w^{\top}Qw$. Then
\[
	w_{t} = \argmin_{w\in \cW} \eta_t \inpro{F_{t-1}}{w} + R(w) = Q^{-1/2} \argmin_{\tilde{w}\in B_1} \eta_t \inpro{Q^{-1/2}F_{t-1}}{\tilde{w}} + \frac{1}{2}\|\tilde{w}\|_2^2, 
\]
and 
\[
R_n = \sum_{t=1}^{n} \inpro{f_t}{w_t} - \min_{w\in\cW}  \inpro{f_t}{w} = \sum_{t=1}^{n} \inpro{Q^{-1/2}f_t}{\tilde{w}_t} - \min_{w\in B_1}  \inpro{Q^{-1/2}f_t}{\tilde{w}}.
\]
Thus, the problem is equivalent to the case of a unit ball constraint set with the loss vector $Q^{-1/2}f_t$ for time $t$, and FTRL with the selected regularizer achieves $O(\sqrt{n})$ worst case regret and $O(\log n)$ regret in the case of an i.i.d.\ loss sequence. Whether FTRL with a constraint-set-independent regularizer $R(w) = \frac{1}{2}\|w\|_2^2$ achieves similar adaptivity, remains an open question.
\end{remark}

\section{Simulations}
\label{sec:Simulations}
We performed three simulations to illustrate the differences between  FTL, FTRL with the regularizer $R(w) = \frac12 \norm{w}_2^2$ when
$w_t = \argmin_{w\in \cW} \sum_{i=1}^{t-1} \ip{f_{i-1},w} + R(w)$,
and the adaptive algorithm $(\cA,\cB)$-prod (AB) using FTL and FTRL as its candidates, which we shall call AB(FTL,FTRL).

For the experiments the constraint set $\cW$ was chosen to be a slightly elongated ellipsoid in the $4$-dimensional Euclidean space, with volume matching that of the $4$-dimensional unit ball.
The actual ellipsoid is given by 
$\cW = \set{w\in \R^4}{w^{\top}Qw \le 1}$
where $Q$ is randomly generated as
\[
Q  = \left(\begin{array}{cccc}
4.3367    & 3.6346   & -2.2250   & 3.5628 \\
3.6346    & 3.9966   & -2.3613   & 3.2817\\
-2.2250   & -2.3613  &  2.0589  & -2.1295\\
3.5628    & 3.2817  & -2.1295  &  3.4206\\
\end{array}\right).
\]

We experimented with 3 types of data to illustrate the behavior of the different algorithms: stochastic, ``half-adversarial'', and ``worst-case'' data (worst-case for FTL), as will be explained below.
The first two datasets are random, so the experiments were repeated 100 times, and we report the average regret with its standard deviation; the worst case data is deterministic, so there no repetition was needed.
For each experiment, we set $n = 2500$. 
The regularization coefficient for the FTRL, and the learning rate for AB were chosen based on their theoretical bounds
minimizing the worst-case regret.
%so as to ensure that the regret is of order $O(\sqrt{n})$.
%We  used the naive $(\cA,\cB)$-prod algorithm of \citet{sani2014exploiting}. 


\paragraph{Stochastic data.}
In this setting  we used the following model to generate $f_t$:
Let  $(\hat{f}_t)_t$ be an i.i.d. sequence drawn from the 4-dimensional standard normal distribution, and let $\tilde{f}_t = \hat{f}_t/\norm{\hat{f}_t}_2$.
Then, $f_t$ is defined as $f_t = \tilde{f}_t  + L e_1$ where $e_1 = (1,0,\dots,0)^\top$. 
Therefore, $\Exp{\norm{\tfrac{1}{t}\sum_{s=1}^t f_s}_2} \to L$ as $t \to \infty$.
In the experiments we picked $L \in \{0, 0.1\}$.

The results are shown in \cref{res:stoch}.
On the left-hand side we plotted the regret against the logarithm of the number of rounds, while on the right-hand side
we plotted the regret against the square root of the number of rounds, together with the standard deviation of the results
over the $100$ independent runs.
As can be seen from the figures,
when $L=0.1$, the growth-rate of the regret of FTL is indeed logarithmic, while when $L=0$, the growth-rate is
$\Theta(\sqrt{n})$. In particular, when $L=0.1$, FTL enjoys a major advantage compared to FTRL,
while for $L=0$, FTL and FTRL perform essentially the same (in this special case, the regret of FTL will indeed 
be $O(\sqrt{n})$ as $w_t$ will stay bounded but $\norm{\Theta_t} = O(1/\sqrt{t})$).
As expected, AB(FTL,FTRL), gets the better of the two regrets with little to no extra penalty.
\todoa{Wouldn't it be enough to provide one picture for both values of $L$ using the relevant scale? I don;t know which one is better.}

\begin{figure}[th]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ExpResults/Stoc_normalized}
	\caption{Regret of FTL, FTRL and AB(FTL,FTRL) against time for stochastic data. \label{res:stoch}}
\end{figure}

\paragraph{``Half-adversarial'' data.}
The half-adversarial data used in this experiment is the optimal solution for the adversary 
in the \emph{linear game} when $\cW$ is the unit ball \citep{abernethy2008optimal}. 
This data is generated as follows:
The sequence $\hat{f}_t$ for $t = 1, \ldots, n$ is generated randomly
in the $(d-1)$-dimensional subspace $S = \text{span}\{e_2, \ldots, e_d\}$ (here $e_i$ is the $i$th unit vector in $\R^d$) as follows:
$\hat{f}_1$ is drawn from the uniform distribution on the unit sphere of $S$ (actually $\bS^{d-1}$. 
For $t = 2, \ldots, n$, $\hat{f}_t$ is drawn from the uniform distribution on the unit sphere
of the intersection of $S$ and the hyperplane perpendicular to $\sum_{i=1}^{t-1} \hat{f}_i$ and going through the origin.
Then, $f_t = Le_1 + \sqrt{1-L^2} \hat{f}_t$ for some $L \ge 0$.
%Note that for $L>0$, the resulting sequence is the one used in proving our logarithmic lower bound.   A: Not anymore

The results are reported in \cref{res:adver}.
When $L=0$,  the regret of both FTL and FTRL grows as $O(\sqrt{n})$. 
When $L=0.1$, FTL achieves $O(\log n)$ regret,
while the regret of FTRL appears to be $O(\sqrt{n})$. 
AB(FTL,FTRL) closely matches the regret of FTL.

\begin{figure}[th]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ExpResults/Adve}
	\caption{Experimental results for ``half-adversarial'' data. \label{res:adver}}
\end{figure}


\paragraph{Worst-case data.}
We also tested the algorithms on data where FTL is known to suffer linear regret, mainly to see how well AB(FTL,FTRL) is able to deal with this setting.
In this case, we set $f_{t,i}=0$ for all $t$ and $i\ge 2$, while 
for the first coordinate, $f_{1,1} = 0.9$, and $f_{t,1} = 2(t \mod 2) - 1$ for $t \ge 2$.

The results are reported in \cref{res:worst_case}. It can be seen that the regret of FTL is linear (as one can easily verify theoretically), and 
AB(FTL,FTRL) succeeds to adapt to FTRL, and they both achieve a much smaller $O(\sqrt{n})$ regret. 

\begin{figure}[th]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ExpResults/WorstCase_alt_new}	
	\caption{Experimental results for worst-case data. \label{res:worst_case}}
\end{figure}


\paragraph{The unit ball.}
We close this section by comparing the performance of our adaptive algorithms on the unit ball, namely, FTL, FTSL, FTLR, and AB(FTL,FTRL). All these algorithms are parametrized as above. The problem setup is similar to the stochastic data setting and the worst-case data setting. Again, we consider a 4-dimensional setting, that is, $\cW$ is the unit ball in $\R^4$ centered at the origin.
The worst-case data is generated exactly as above, while the generation process of the stochastic data is slightly modified to increase the difference between FTLR and FTL: we sample the i.i.d. vectors $\hat{f}_t$ from a zero-mean normal distribution with independent components whose variance is $1/16$, and let $\tilde{f_t}=\hat{f}_t$ if $\|\hat{f}_t\|_2 \le 1$ and $\tilde{f}_t = \hat{f}_t/\norm{\hat{f}_t}_2$ when $\norm{\hat{f}_t}_2>1$ (i.e., we only normalize if $\hat{f}_t$ falls outside of the unit ball).
The reason of this modification is to encourage the occurrence of the event $\|F_{t-1}\|_2 < \sqrt{t-1}$. Recall that when $\|F_{t-1}\|_2 \ge \sqrt{t-1}$, the prediction of FTRL matches that of FTL, so we are trying to create some data where their behavior is actually different. As a result, we will be able to observe that the predictions of FTL and FTRL are different in the early rounds. Finally, as before, we let $f_t=\tilde{f}_t + L e_1$, and set the time horizon to $n=20,000$.
 

The results of the simulation of the stochastic data setting are shown in Figure~\ref{res:Stoc_unitBall}. In the case of $L=0.1$, FTRL suffers more regret at the beginning for some rounds, but then succeeds to match the performance of FTL.
The results of the simulation of the worst-case data setting are shown in Figure~\ref{res:WorstCase_unitBall}, where FTSL has similar performance as FTRL.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ExpResults/Stoc_unitBall}	
	\caption{Experimental results for stochastic data when $\cW$ is the unit ball. \label{res:Stoc_unitBall}}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ExpResults/WorstCase_unitBall}	
	\caption{Experimental results for worst-case data when $\cW$ is the unit ball. \label{res:WorstCase_unitBall}}
\end{figure}



\section{Conclusion}
FTL is a simple method that is known to perform well in many settings, while 
existing worst-case results fail to explain its good performance.
While taking a thorough look at why and when FTL can be expected to achieve small regret, we discovered that the curvature of the boundary of the constraint and having average loss vectors bounded away from zero help keep the regret of FTL small. These conditions are significantly different from previous conditions on the curvature of the loss functions which have been considered extensively in the literature.
It would be interesting to further investigate this phenomenon for other algorithms or in other learning settings.
%We suspect that other algorithms can also take advantage of this, opening up interesting new 
%Amongst the numerous possible follow-up questions, perhaps the most interesting would be to design 
%a version of FTRL where the regularization coefficient would be chosen adaptive to interpolate between the performance
%of FTL and FTRL in a data-dependent fashion. While such a result was achieved with 
%$(\cA,\cB$)$-prod, 


\appendix
\section{Technical results}
%\section{Appendix: Technical results}
\subsection{Proof of \cref{prop:derivativePhi}}
\label{sec:pr-prop:derivativePhi}
Under the extra condition that $\cW$ is compact
the result follows from Danskin's theorem (e.g., Proposition B.25 of \citealt{bertsekas99nonlinear}).
However, compactness is not required. \todoc{In fact, I don't get why Bertsekas needs it for this part of the statement. He may need it elsewhere.}
For completeness, we provide a short, direct proof. 
\todoc{Probably delegate to the appendix.}
We need to show that 
$\cZ = \partial \varphi(\Theta)$ where recall that
\begin{align*}
\partial \varphi(\Theta)= \set{u\in \R^d}{ \varphi(\Theta) + \inpro{u}{\cdot-\Theta} \le \varphi(\cdot)}
= \set{u\in  \R^d}{ \varphi(\Theta)  \le \inpro{u}{\Theta} + \varphi(\cdot) - \inpro{u}{\cdot} }\,.
\end{align*}
Since $\cZ \subset \cW$, 
if $w\in \cZ$, $\varphi(\Theta') \ge \ip{w,\Theta'}$ for any $\Theta'$ by the definition of $\varphi$.
Hence, $\varphi(\Theta) = \ip{w,\Theta} \le \ip{w,\Theta} + \varphi(\Theta')-\ip{w,\Theta'}$ for any $\Theta'$, implying that $w\in \partial \varphi(\Theta)$.

On the other hand, assume $w \in \partial \varphi(\Theta)$. Then $\varphi(\Theta) \le \ip{w,\Theta}$
since $\varphi(0) = \ip{w,0} = 0$. 
%Now notice that  $\varphi(\lambda \Theta) = \lambda \varphi(\Theta)$ holds for any $\lambda>0$.
%Using this and that  $\varphi(\Theta') \ge \varphi(\Theta) + \ip{w,\Theta'-\Theta}$ with $\Theta'=2\Theta$,
%we get $\ip{w,\Theta}\le \varphi(\Theta)$. Hence, $\ip{w,\Theta} = \varphi(\Theta)$.
Since $\cW$ is closed, $\cZ$ is also closed. Therefore, if $w \not\in\cZ$,
the strict separation theorem (applied to $\{w\}$, a convex compact set,
and $\cZ$, a convex closed set) implies that
 there exists $\rho\in \R^d$ such that $\ip{z,\rho} < \ip{w,\rho}$ for all $z\in \cZ$.
 Let $\Theta' = \Theta +  \rho$.
 Then, $\varphi(\Theta') = \max_{u\in \cW} \ip{u,\Theta} +  \ip{u,\rho}
 < \varphi(\Theta) +  \ip{w,\Theta'-\Theta} \le \ip{w,\Theta'} \le \varphi(\Theta')$, a contradiction.
Hence, $w\in \cZ$.



\subsection{Preliminaries in differential geometry}
\label{subsec:preliminariesDiffGeo}

In this section we present some extra details on the tools we use from differential geometry. We focus on providing the intuitive picture and some of the foundational results we use in the paper, and omit the formal definitions that not directly contribute to this goal; the interested reader can find a detailed formal treatment, for example, in Section~2.5 of the book of \citet{Sch14:ConvexBodies}. 
%In particular, we will omit the formal definition of a manifold; for the purposes of this paper it is sufficient to think of a manifold as the boundary of a convex body in $\R^d$  

\subsubsection{Planar curves}
\label{sec:app_planar}
We only consider twice continuously differentiable curves in this paper, defined as injective twice continuously differentiable functions $\gamma\,:\,  [a,b] \rightarrow X$ from an interval $[a,b]\subset \real$ to a differentiable manifold $X \subset \real^d$  such that $\gamma'(u) \neq 0$.  
Given a differentiable curve $\gamma$, we define its length between $\gamma(u)$ and $\gamma(v)$ as $\ell([u,v]) = \int_{u}^{v} \|\mathbf{\gamma}'(s)\| ds$ for $u,v \in [a,b]$; throughout this section $\|\cdot\|$ denotes the Euclidean norm, and--with a slight abuse of notation--we will also use $\ell$ to denote the length of a curve or an interval.
Given a twice continuously differentiable bijective mapping $r\,:\, [a,b] \rightarrow J$ (where $J \subset \real$ is an interval), $\gamma$ can be reparametrized as a twice continuously differentiable function from $J$ to $X$, by $\tilde{\gamma}(\ell) = \gamma(r^{-1}(\ell))$.
In particular, when the mapping is $r(u) = \ell([a,u])$, that is, the curve is reparametrized by the curve length, we have $r^{-1}(\ell([a,u]))=u$ and $r'(u) = \|\gamma'(u)\| >0$. 
Moreover, since $\ell = r(r^{-1}(\ell))$, we also have $1 = \frac{d\, r(r^{-1}(\ell))}{d\, \ell} = r'(u)\frac{d\, r^{-1}(\ell)}{d\, \ell}$
where $r(u) = \ell$. Thus, $\frac{d\, r^{-1}(\ell)}{d\, \ell} = \frac{1}{r'(u)} = \frac{1}{\|\gamma'(u)\|}$, and so
\[
\|\tilde{\gamma}'(\ell)\| = \left\|\frac{d\, \gamma(r^{-1}(\ell))}{d\, \ell}\right\| =\left|\frac{d\, r^{-1}(\ell)}{d\, \ell}\right|\,  \left\|\gamma'(u)\right\| = 1.
\] 
Thus, if the curve is parametrized by the its length, its gradient is always a unit vector. For the rest of this section, we always assume this parametrization. %at the curve is parametrized in its curve length.

A planar curve is a curve in a 2-dimensional plane. Given a point $\gamma(u)$ on the curve, one can compute its tangent vector in the plane by $\gamma'(u)$. Note that since $\gamma$ is parametrized by its curve length, $\gamma'(u)$ is a unit vector. To measure how \emph{curved} a planar curve is at point $\gamma(u)$, we define its curvature as 
\[
\kappa(\gamma(u)) = \left\| \frac{d\, \gamma'(u)}{d\, u}\right\| = \| \gamma''(u)\|_2. 
\]
Furthermore, since $\gamma'(u)$ is a unit vector, 
\[
0 = \frac{d\, 1}{d\, u} = \frac{d\, \|\gamma'(u)\|^2}{d\, u} = \frac{d\, \langle \gamma'(u),\, \gamma'(u)\rangle}{d\, u} = 2\langle \gamma'(u) ,\, \gamma''(u)\rangle,
\] 
thus $\gamma'(u)$ is perpendicular to $\gamma''(u)$. 

\subsubsection{Manifolds, tangent plane, and principal curvature}
\label{sec:app_manifold}
A manifold $M$ of dimension $d$ is a Hausdorff topological space that is locally homeomorphic to $\real^d$. 
%For any point $w \in M$, there exists an open set $\Omega$ that $w\in \Omega$ and a homeomorphism $\psi$ from $\Omega$ onto an open set of $\real^d$. 
%Such pair of $(\Omega, \psi)$ is called a local chart. 
%A collection of charts $(\Omega_i,\, \psi_i)$ such that $M = \cup_i \Omega_i$ is called an atlas of $M$. 
%Based on the definition of charts, for two local charts $(\Omega_1,\, \psi)$ and $(\Omega_2,\, \psi_2)$ such that $\Omega_1 \cap \Omega_2 \neq \emptyset$, the transition map $\psi_2\circ\psi_1^{-1}$ is a diffeomorphism from $\psi_1(\Omega_1\cap\Omega_2)$ to $\psi_2(\Omega_1\cap\Omega_2)$. 
%An atlas is called a $C^k$ atlas if all the transition maps are $C^k$. 
%We then define the equivalence between atlases:
%two $C^k$ atlases $\{(\Omega_i, \psi_i)\}_{i\in I}$ and $\{(U_j, \eta_j)\}_{j\in J}$ are equivalent if and only if $\psi_i\circ\eta_j^{-1}$ is $C^k$ when $\Omega_i\cap U_j \neq \emptyset$.
%Finally a $C^k$ manifold is defined to be a manifold together with an equivalent class of $C^k$ atlases. 
%One can further define the concept of submanifold, such that a curve on the manifold $M$ is a $M$'s submanifold of dimension $1$. We omit the rigorous definitions of this part.
%A manifold with boundary $M$ is defined in a similar way to manifolds, except that it is locally homeomorphic to $\real_{+}^d$, where $\real_{+}^d$ is the half sapce $x_1\ge0$ of $\real^d$. 
%A point $w \in M$ is called an interior point, if its neighborhood is homeomorphic to an open set in $\real^d$. A point $w\in M$ is called a boundary point, if $w$ is not an interior point.
%The set of all boundary points is denoted by $\partial M$. One can prove that $\partial M$ is a manifold of dimension $d-1$.
%In particular, given $w\in \partial M$ and a local chart $\psi$ of $w$, $\psi(w) \in \Pi$ where $\Pi = \{x\,\vert\, x_1=0\}$ is the $d-1$ dimensional hyperplane in $\real^d$.
%  
Given a convex body (a convex body is a compact, convex subset of $\R^d$ with non-empty interior) $\cW \subset \real^d$, its boundary is a manifold $M = \bd(\cW)$ of dimension $d-1$. Assume that $M$ is twice continuously differentiable, and let $\zeta$ denote the standard embedding map from $M$ to $\cW$.%
\footnote{A detailed discussion of manifolds including local charts and atlases can be found in Section~2.5 of the book of  \citet{Sch14:ConvexBodies}, together with a formal definition of differentiability. To build the intuition required to follow the arguments in the paper it is sufficient to think of a manifold as the boundary of a convex body.}
%For a point $w \in M$,  assume that $\phi \,: U \rightarrow \real^{d}_{+}$ is a local chart of $\cW$ at point $w$. 
Now let $\gamma_1,\gamma_2\,: [-1,1] \rightarrow M$ be two curves (not necessarily parametrized by curve length),  such that $\gamma_1(0) = \gamma_2(0)=w$. The two curves are equivalent at the point $w$ if and only if their derivatives are equal at the point $u$, that is, $(\zeta\circ\gamma_1)'(0) = (\zeta\circ\gamma_2)'(0)$ and the tangent vector embedded in $\real^d$ associated with this equivalence class is $(\zeta\circ\gamma_1)'(0)$. The set of all the tangent vectors form the tangent space, denoted by $T_{w}M$ or $T_w \cW$. 
%Equivalently, each equivalence class can be embedded into $\R^d$ and represent
%we call this equivalent class of curves a tangent vector at point $w$ denoted by $\tilde{\gamma}_w$. The tangent vector can be embedded into $\real^d$ by mapping it to the derivative vector $(\zeta\circ\gamma_1)'(0)$, which we will also call a tangent vector, and the set ofWe then define $d\,\phi_w$ from $T_wM$ to $\real^{d-1}$ by $d\,\phi_w(\tilde{\gamma}_w) =(\zeta\circ\gamma_1)'(u)\vert_{u=0}$
One can verify that $T_wM$ is a $d-1$ dimensional hyperplane in $\real^d$. Note that this definition of tangent space is consistent with the `natural' tangent plane in the Euclidean space $\real^3$.%
\footnote{More generally, one can define the tangent space without embedding it to $\real^d$, but, for simplicity, we only consider here the definitions through the embedding $\zeta$, which is always possible and allows to perform calculations in $\real^d$.}
%In general, we can embed all the mentioned objects in the Euclidean space $\real^d$ in a compatible way, and thus perform the calculation in $\real^d$.

Since $T_w M$ is a $d-1$ dimensional hyperplane in $\real^d$, there exists a unique vector that is perpendicular to $T_wM$, is of length 1 and points outward of $\cW$ (note that in this sense $M=\bd(\cW)$ is oriented): this vector is called the Gauss vector at point $w$ for $\cW$. The mapping $u_{\cW}:\bd(\cW) \rightarrow \bS^{d-1}$ that maps every $w\in \bd(\cW)$ to the corresponding Gauss vector is called the Gauss map. Since $M$ is twice continuously differentiable, the Gauss map $u_{\cW}$ is continuously differentiable.
One can actually show that $\nabla u_{\cW}(w)$, the so-called Weingarten map, is a self-adjoint operator with nonnegative eigenvalues, which can be represented as a $(d-1)\times (d-1)$ positive semidefinitive matrix.
The eigenvalues of this matrix (or the self-adjoint operator) are called the principal curvatures of $M$ at point $w$. Intuitively, how fast the Gauss map $u_{\cW}$ changes characterizes the curvature of the manifold $M$. An interesting property of the operator $\nabla u_{\cW}(w)$ is that it maps $T_w M$ to itself: for any unit vector $v\in T_wM$, $0 = \lim_{\epsilon\rightarrow 0} \frac{\partial 1}{\partial \epsilon} = \lim_{\epsilon\rightarrow 0}\frac{\partial \|u_{\cW}(w+\epsilon v)\|_2^2}{\partial \epsilon} = 2\lim_{\epsilon\rightarrow 0}\langle \nabla u_{\cW}(w+\epsilon v)v,\, u_{\cW}(w+\epsilon v)\rangle = 2\langle \nabla u_{\cW}(w)v,\, u_{\cW}(w)\rangle$ , thus $\nabla u_{\cW}(w)v$ is perpendicular to $u_{\cW}(w)$, thus belongs to $T_wM$.

\subsection{Technical proofs related to strongly convex sets and principal curvatures}
\label{app:stronglyconvexity}
%\FTLSCset*
\begin{proofof}{\cref{prop:strongconvex}}
We show that \eqref{sc:l1} implies \eqref{sc:l2}, \eqref{sc:l2} implies \eqref{sc:l3}, and \eqref{sc:l3} implies \eqref{sc:l1}. 

We start with showing that \eqref{sc:l1} implies \eqref{sc:l2}.
First note that all principal curvatures of the $d$-dimensional ball $B=B_{1/\lambda}$ with radius $1/\lambda$ (centered at the origin) are $\lambda$. Therefore, \eqref{sc:l1} and Theorem~3.2.9 of \citet{Sch14:ConvexBodies} implies that there is a convex body $\cM$ such that $\cW+\cM=B$, where for two sets, $S_1, S_2 \subset \R^d$, $S_1+S_2$ is defined as $\set{s_1+s_2}{s_1 \in S_1, s_2\in S_2}$. For any $\theta \in \bS^{d-1}$, let $m_\theta \in \argmax_{m \in \cM} \ip{m,\theta}$. Then clearly $w_\theta+m_\theta$ maximizes $\ip{b,\theta}$ for $b \in \cW+\cM$. Therefore, $\cW + m_\theta$ is a subset of $B$ and touches it at $w_\theta+m_\theta$, or equivalently $\cW \subset B-m_\theta$ and they touch each other, and a tangent hyperplane with normal vector $\theta$, in $w_\theta$. This proves that \eqref{sc:l1} implies \eqref{sc:l2}. 
%The other direction is a trivial consequence of Corollary~3.2.10 of \citet{Sch14:ConvexBodies}.

Next we prove that  \eqref{sc:l2} implies \eqref{sc:l3}. Assuming \eqref{sc:l2} holds, let $w \in \cW$ be any point in the interior of $\cW$, and let $p \in \bd(\cW)$ be the closest boundary point to $w$, and recall that $T_p\cW$ is the tangent space of $\cW$ at $p$. By construction, $B_{\|w-p\|_2}(w)$ touches the boundary of $\cW$ at $p$ (in the sense that they do not intersect, but they can have multiple common points), and so $w-p$ is orthogonal to $T_p\cW$. Therefore,  $B_{\|w-p\|_2}(w)$ also touches the boundary of the ball $B=B_{1/\lambda}(p+\frac{w-p}{\lambda \|w-p\|_2})$, which contains $\cW$ by assumption \eqref{sc:l2}. Now consider any two points $x,y \in \cW$ and $\gamma \in [0,1]$ such that $w=\gamma x + (1-\gamma) y$. Then the ball with radius $\lambda \gamma (1-\gamma) \|x-y\|_2^2/2$ centered at $w$ is contained in $B$, since $B$ is $\lambda$-strongly convex. But then its radius is at most $\|p-w\|_2$, and so it is also contained in $\cW$. This shows that $\cW$ is $\lambda$-strongly convex, thus  \eqref{sc:l3} holds.
\begin{wrapfigure}{R}{0.4\textwidth}
	\vspace{-.7cm}
	\begin{framed}
		%\begin{figure}[t]
		\centering
		\includegraphics[width=\textwidth]{figures/localCoordinate}
		\caption{The local coordinate system at $w$. \label{fig:stronglyconvexset}}
		%\end{figure} 
	\end{framed}
	\vspace{-0.5cm}
\end{wrapfigure}

To finish the proof of the proposition, assume \eqref{sc:l3}. To prove that \eqref{sc:l1} holds, we have to show that for any point $w$ on $\bd(\cW)$ and for any unit vector $v \in T_w\cW$, the curvature of the boundary along $v$ is at least $\lambda$.
Using the same notations as in \cref{fig:diffgeo} in \cref{subsec:positiveCurvature}, 
let $P$ be the hyperplane spanned by $v$ and the outer normal vector $u_{\cW}(w)$ of $\cW$ at point $w$, and consider the planar curve $\gamma$ defined by $\bd(\cW) \cap P$.
Using $v$ as the axis of a local 2-dimensional coordinate system, a point $\gamma(s)$ on the curve $\gamma$ in the neighborhood of $w$ can be expressed as $\gamma(s) = w + sv - f(s)u_{\cW}(w)$, where $w$ serves as the origin in the local coordinate system, $f$ is the restriction of the function $f_w(sv)$ (see \cref{subsec:positiveCurvature}) to $P$ (to simplify the notation, we denote it by $f(s)$, omitting $v$ and $w$), and the curve $\gamma$ is the epigraph of the function $f$, as in \cref{fig:stronglyconvexset}.

Note that $f'(0)=0$, and by Proposition 2.1 of \citet{pressley2010elementary}, the curvature of $\gamma$ at $p$ can be obtained as 
\[
\frac{f''(s)}{\sqrt{1+f'(s)^2}^3}\Bigg\vert_{s=0} = f''(0)~.
\]
Now since $w(s),w(-s) \in \cW$ for a sufficiently small $s$, the strong convexity of $\cW$ applied to $w(s)$ and $w(-s)$ with $\gamma=1/2$
implies that $q=\frac{w(s)+w(-s)}{2}+\frac{\lambda}{8} \|w(s)-w(-s)\|_2^2 u \in \cW$. Substituting the definition of $w(s)$ and $w(-s)$, we get
\[
q=p - u \left[\frac{f(s)+f(-s)}{2} - \frac{\lambda}{8} \Bigl(4s^2 + (f(s)-f(-s))^2\Bigr)\right].
\]
Therefore, $q \in \cW$ implies
$f(s)+f(-s) \ge \lambda s^2$, and so
\[
	f''(0) = \lim_{s \ra 0} \frac{\frac{f(s) - f(0)}{s} - \frac{f(0) - f(-s)}{s}}{s} = \frac{f(s)+ f(-s)}{s^2} \ge \lambda.
\]
Thus \eqref{sc:l1} holds, finishing the proof of the proposition. 
\end{proofof}

\newcommand{\thetagg}{\theta_{\gamma}}
\begin{proofof}{a weakened variant of \eqref{eq:middletheta} based on strong convexity} \\
Given $\theta_1$ and $\theta_2$, for any $0<\gamma< 1$, define $\thetagg = \gamma \theta_1 + (1-\gamma)\theta_2$ and
\[
   w_{\gamma} = \gamma w^{(1)} + (1-\gamma) w^{(2)} + \frac{\lambda_0}{2}\gamma(1-\gamma) \|w^{(1)} - w^{(2)}\|_2^2\frac{\thetagg}{\|\thetagg\|_2}~.
\]
By the strong convexity of $\cW$, $w_\gamma \in \cW$, and so by the definition and convexity of the support function $\Phi$, we have
\[
  \inpro{w_{\gamma}}{\thetagg} \le \Phi(\thetagg) \le \gamma \Phi(\theta_1) + (1-\gamma) \Phi(\theta_2) = \gamma \inpro{w^{(1)}}{\theta_1} + (1-\gamma) \inpro{w^{(2)}}{\theta_2}.
 \]
Plugging in the definitions of $w_{\gamma}$ and $\thetagg$, and applying the Cauchy-Schwarz inequality, we obtain
\[
   \frac{\lambda_0}{2} \gamma (1-\gamma) \|w^{(1)} - w^{(2)}\|_2^2\|\thetagg\|_2 \le	\gamma(1-\gamma) \inpro{\theta_1 - \theta_2}{w^{(1)} - w^{(2)}}
   \le \gamma(1-\gamma)\|\theta_1 - \theta_2\|_2 \|w^{(1)} - w^{(2)}\|_2~.
\]
Rearranging and letting $\gamma \rightarrow 0$ implies
\[
	\|w^{(1)} - w^{(2)}\|_2 \le \frac{2}{\lambda_0}\frac{\|\theta_1 - \theta_2\|_2}{\|\theta_2\|_2}\,.
\]
Finally, the definition of $w^{(2)}$ and the Cauchy-Schwarz inequality yields
\begin{align}
\inpro{w^{(1)} - w^{(2)}}{\theta_1} &\le \inpro{w^{(1)} - w^{(2)}}{\theta_1} + \underbrace{\inpro{w^{(2)} - w^{(1)}}{\theta_2}}_{\ge 0}
= \inpro{w^{(1)} - w^{(2)}}{\theta_1 - \theta_2} \nonumber \\
\label{middletheta2}
& \le \|w^{(1)} - w^{(2)}\|_2 \|\theta_1 - \theta_2\|_2 \le \frac{2}{\lambda_0} \frac{\|\theta_1 - \theta_2\|_2^2}{\|\theta_2\|_2}\,, 
\end{align}
finishing the proof.
\end{proofof}

\begin{proofof}{\cref{ex:curvature}}
We start with proving the last statement, part \eqref{it:ex1gen}, which implies the rest.
Fix $w\in\bd(\cW)$. Note that $\phi'(w)$ is a normal vector at $w$ for $\bd(\cW)$, thus $T_w\cW = \seto{v: \inpro{v}{\phi'(w)}}$.
Then the Gauss map $u_{\cW}$ of $\cW$ satisfies $u_{\cW}(w) = \frac{\phi'(w)}{\|\phi'(w)\|_2}$ for $w\in\bd(\cW)$.
According to \citet[page 105]{Sch14:ConvexBodies}, the principal curvatures of $\cW$ at $w$ are the eigenvalues of the Weingarten map $W_w(v)$, which is a linear map from $T_w\cW$ to itself defined through the derivative of $u_{\cW}$: $W_w(v)=\inpro{\frac{d u_{\cW}}{d w}}{v}$. In our case,
\[
W_w(v) = \inprol{\frac{d u_{\cW}}{d w}}{v}=  \frac{\nabla^2\phi(w)v}{\|\phi'(w)\|_2} -\frac{\phi'(w)\nabla^2\phi(w)\phi'(w)^{\top}v}{\|\phi'(w)\|_2^3} = \frac{\nabla^2\phi(w)v}{\|\phi'(w)\|_2}~,
\]
where in the last step we used that $\phi'(w)$ is orthogonal to the tangent space $T_w \cW$ (since it is parallel to the normal vector $u_\cW(w)$), and $v \in T_w \cW$.
Therefore, the smallest principal curvature at $w$ is the smallest eigenvalue $\min_{v \in \bS^{d-1}: \inpro{\phi'(w)}{v}=0} \frac{v^{\top}\nabla^2\phi(w) v}{\|\phi'(w)\|_2}$. Taking minimum over all  $w\in\bd(\cW)$ finishes the proof. 

Now part \eqref{it:ex1Q} follows for $\phi(w)=w^\top Q w$, as we need to minimize $v^\top Q v/\| w^\top Q\|_2$. It is easy to see that the denominator is maximized when $w \in \bd(\cW)$ is an eigenvector of $Q$ corresponding to $\lambda_{\max}$ (with length $1/\sqrt{\lambda_{\max}}$), and the numerator is minimized (for arbitrary $v \in \bS^{d-1}$) when $v$ is an eigenvector of $Q$ corresponding to $\lambda_{\min}$. Since the two eigenvectors, $w$ and $v$ are orthogonal (or can be chosen to be orthogonal if they are not unique), $v$ is orthogonal to $\phi'(w)= Q w = \lambda_{\max} w$, and hence it is a valid minimizer. This completes the proof of part \eqref{it:ex1Q}, and part \eqref{it:ex1r} follows as a special case.

Part \eqref{it:ex1Lp} follows similarly: Due to symmetry, it is enough to consider $w$ in the nonnegative quadrant (i.e., $w_i \ge 0$ for all $i$). Calculating the first and second derivatives of $\phi(w)=\|w\|_p = \left(\sum_{i=1}^{d}w_i^p\right)^{1/p}$, for any $w \in \bd(\cW)$ (i.e., $\|w\|_p=1$), we obtain
\[
\phi'(w) = \left(\sum_{i=1}^{d} w_i^p\right)^{1/p-1}w^{\odot (p-1)} = w^{\odot(p-1)}
\quad \text{and} \quad
\nabla^2\phi(w) =  (p-1) \diag\left(w_1^{p-2},\cdots,w_d^{p-2}\right).
\]
When $p>2$, picking $w = (1,0,0,\cdots, 0)$, one can easily verify that $\lambda_0 = 0$. For $1<p\le 2$, $|w_i|^{p-2} \ge 1$ since $|w_i| \le 1$ by the assumption that $\|w\|_p = 1$. Thus, the minimum eigenvalue of $\diag\left(w_1^{p-2},\cdots,w_d^{p-2}\right)$ is at least $1$, and so $\lambda_0 \ge (p-1)/\|w^{\odot(p-1)}\|_2$ since $ v^\top \diag\left(w_1^{p-2},\cdots,w_d^{p-2}\right) v \ge 1$.
Defining $q$ via $1/p+1/q=1$, for any $w \in \bd(\cW)$ (i.e., with $\|w_p\|=1$), H\"older's inequality implies
\[
\|w^{\odot(p-1)}\|_2 \le d^{\frac{1}{2}-\frac{1}{q}} \|w^{\odot(p-1)}\|_q = d^{\frac{1}{2}-\frac{1}{q}}  \left(\sum_{i=1}^d w_i^{(p-1)q}\right)^\frac{1}{q}
= d^{\frac{1}{2}-\frac{1}{q}}  \left(\sum_{i=1}^d w_i^{p}\right)^\frac{1}{q} = d^{\frac{1}{2}-\frac{1}{q}}  = d^{\frac{1}{p}-\frac{1}{2}}~.
\]
Thus, $\lambda_0 \ge (p-1)d^{\frac{1}{2}-\frac{1}{p}}$, as desired.
\end{proofof}



\subsection{Technical lemmas for the lower bound, \cref{thm:lowerbound}}
\label{sec:pr-lowerbound}

\begin{lemma} Under the assumptions of \cref{thm:lowerbound}, for any $0<P_1,P_2<1$,
	\label{lem:P2P1loss}
	\[
	\inner{w^{P_2} - w^{P_1}, f^{P_1}} \ge \frac{\lambda L}{2} \frac{\left( \frac{2P_2 - 2P_1}{\lambda L} \right)^2}{\sqrt{1+\left( \frac{1-2P_1}{\lambda L}\right)^2 } \left(1+\left( \frac{1-2P_2}{\lambda L}\right)^2 \right)}.
	\]
\end{lemma}
\begin{proof}
	It is easy to see that for any $p$, $w^p$ is on the boundary of $\cW$, that is, $w^p = \argmin_{w\in\cW} \inner{ w, f^p } = (\cos (\varphi^p), \lambda\sin (\varphi^p))$ for some $\varphi^p$. 
	Then $\inner{w^p,f^p}= (2p-1) \cos (\varphi^p) - \lambda L \sin (\varphi^p)$, and so taking the derivative it is easy to verify that $\tan(\varphi^p) = \frac{\lambda L}{1-2p}$ and $\sin(\varphi^p) = \frac{\lambda L}{\sqrt{(\lambda L)^2+(1-2p)^2}} >0$. 
	Thus, $1-2P_1 = \frac{\lambda L\cos (\varphi^{P_1})}{\sin (\varphi^{P_1})}$. To simplify notation, let $\varphi_1=\varphi^{P_1}$ and $\varphi_2=\varphi^{P_2}$. Then,
	\begin{align}
	\langle w^{P_2} - w^{P_1}, f^{P_1} \rangle & = \left\langle \left(
	\begin{array}{c}
	\cos \varphi_{2} - \cos \varphi_{1} \\
	\lambda \left(\sin \varphi_{2} - \sin \varphi_{1} \right) 
	\end{array}
	\right),  \left( 
	\begin{array}{c}
	\frac{-\lambda L\cos \varphi_{1}}{\sin \varphi_{1}} \\
	-L
	\end{array}
	\right) \right\rangle \nonumber \\ 
	& = -\lambda L\left( \left( \cos (\varphi_{2}) - \cos (\varphi_{1} ) \right) \frac{\cos (\varphi_{1})}{\sin( \varphi_{1})} 
		 + \left(\sin (\varphi_{2}) - \sin (\varphi_{1} )\right)  \right)  \nonumber \\
	& = \frac{-\lambda L}{\sin( \varphi_{1})} \left( \cos (\varphi_{2} )\cos (\varphi_{1} )- \cos^2(\varphi_{1} )+ \sin (\varphi_{1}) \sin (\varphi_{2}) - \sin^2(\varphi_{1}) \right)  \nonumber \\
	& = \frac{\lambda L}{\sin (\varphi_{1})} \left( 1- \cos (\varphi_{2}) \cos (\varphi_{1})  - \sin (\varphi_{1}) \sin (\varphi_{2}) \right) \nonumber \\
	& = \frac{\lambda L}{\sin (\varphi_{1})} \left( 1- \cos(\varphi_{1}-\varphi_{2}) \right) \nonumber \\
	& = \frac{\lambda L}{\sin (\varphi_{1})} \left(\frac{1}{2}\left(\cos(\varphi_{1}-\varphi_{2})-1\right)^2 + \frac{1}{2} \sin^2 (\varphi_{1}-\varphi_{2})\right) \\
	& \ge  \frac{\lambda L}{2 \sin (\varphi_{1})}  \sin^2 (\varphi_{1}-\varphi_{2}) \nonumber \\
	& = \frac{\lambda L}{2} \sin (\varphi_{1}) \sin^2 \varphi_{2} \left(\cot (\varphi_{1} )- \cot (\varphi_{2})\right)^2~. % \\
%	& = \frac{\lambda L}{2}  \frac{\left( \frac{2P_2 - 2P_1}{\lambda L} \right)^2}{\sqrt{1+\left( \frac{1-2P_1}{\lambda L}\right)^2 } \left(1+\left( \frac{1-2P_2}{\lambda L}\right)^2\right)},
	\end{align}
	The proof is finished by substituting $\cot (\varphi_i) = \frac{1-2P_i}{\lambda L}$, $\sin(\varphi_1) = \frac{1}{\sqrt{1+\left(\frac{1-2P_1}{\lambda L}\right)^2}}$ and $\sin^2 (\varphi_2) =   \frac{1}{1+\left(\frac{1-2P_2}{\lambda L}\right)^2}$.
\end{proof}

\begin{lemma}[Concentration of $\hP_{t}$] For any $u>0$,
	\label{lem:concenPhat}
	\[
	\Probc{|\hat P_{t}-P| > \frac{K}{2K+t}|1-2P| + \frac{t}{2K+t}u}{P} \le 2\exp(-tu^2)~.
	\]
\end{lemma}
\begin{proof}
	Recall that $\hat P_t = \frac{K+\sum_{i=1}^{t}X_i}{2K+t}$. Thus, 
	\begin{align} 
	\Probc{|\hat P_{t}-P| > u }{P} & = \Probc{\left| \frac{K+\sum_{i=1}^{t}X_i}{2K+t} -P\right| > \frac{K}{2K+t}|1-2P| + \frac{t}{2K+t}u}{P} \nonumber \\ 
	& =  \Probc{\left| \sum_{i=1}^{t}X_i - Pt + K(1-2P) \right| > K|1-2P|+ tu }{P} \nonumber \\ 
	& \le \Probc{\left| \sum_{i=1}^{t}X_i - Pt \right| > tu }{P}, \label{eq:hoeffding}
	\end{align}
	where the last inequality is due to $\Prob{|A+b|>c} \le \Prob{|A| > c-|b|}$. Note that conditioned on $P$, $X_1, \ldots, X_t$ are independent Bernoulli random variables with expectation $P$, thus \eqref{eq:hoeffding} holds by Hoeffding's inequality (see, e.g., \cite[Corollary~A.1]{CBLu06:book}).
\end{proof}

%\begin{lemma}[Length of $\theta_t$]
%	\label{lem: lengthoftheta}
%	\[
%	L \le \E \left[\|\Theta_t\|_2 \right] \le L+\frac{1}{4K}.
%	\] 
%\end{lemma}
%\begin{proof}
%	Note that the second component of $\theta_t$ is $-L$, thus $\E \left[\|\theta\|_2 \right] \ge L$. For the other inequality, note that 
%	\[
%	\E \left[ \| \frac{1}{t} \sum_{i=1}^{t} f_i\|_2\right] \le \E \left[ \| f_i\|_2\right] = \E[\sqrt{L^2 + (1-2X_i)^2}] \le L + \sqrt{\E[|1-2X_i|^2]} = L+\frac{1}{4K}.
%	\]
%\end{proof}

\begin{lemma}
	\label{lem:bayeserror}
	\[
	\Expc{(P-\hat{P}_t)^2}{P} = \frac{K^2(1-2P)^2}{(2K+t)^2} + \frac{tP(1-P)}{(2K+t)^2}.
	\]
\end{lemma}
\begin{proof}
	Recall that $\hat P_t = \frac{K+\sum_{i=1}^{t}X_i}{2K+t}$.Thus, 
	\begin{align*}
	\Expc{(P-\hat{P}_t)^2}{P} & = \Expc{\left(\frac{K(1-2P)}{2K+t} + \frac{\sum_{i=1}^{t}X_i- Pt}{2K+t}\right)^2}{P} \\
	& = \frac{K^2(1-2P)^2}{(2K+t)^2} + \frac{1}{(2K+t)^2}\Expc{ \left(\sum_{i=1}^{t}X_i - tP\right)^2}{P} \\
	& = \frac{K^2(1-2P)^2}{(2K+t)^2} + \frac{tP(1-P)}{(2K+t)^2},
	\end{align*}
	where the second equality is due to $\Expc{ \sum_{i=1}^{t}X_i - Pt}{P} =0$, and the last equality is due to that conditioned on $P$, $\sum_{i=1}^{t}X_i$ has a Binomial distribution with parameters $t$ and $P$.
\end{proof}

\section*{Acknowledgements}
The authors would like to thank the anonymous reviewers for their comments that helped improving the presentation of the paper, in particular, for the alternative proof of \eqref{eq:middletheta} based on strong convexity. This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and by NSERC.
During part of this work, T. Lattimore was with the Department of Computing Science, University of Alberta and with the  School of Informatics and Computing, Indiana University  Bloomington.

%\small
\setlength{\bibsep}{0.4\bibsep}
\bibliography{reference}
%\bibliographystyle{plainnat}
%\include{appendix}
%\include{otherResults}
\end{document}




